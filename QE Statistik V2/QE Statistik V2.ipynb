{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library-library\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation and Preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "# Word Embedding\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and Expansion Query\n",
    "import nltk\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from yake import KeywordExtractor\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "NLTK_StopWords = stopwords.words('indonesian')\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = [\"tempat\", \"waktu\"]\n",
    "NLTK_StopWords.append([\"detik\", \"detikjatim\", \"detikjateng\", \"detikjabar\", \"detiksulsel\", \"detiksumbar\", \"detikbali\", \"detikpapua\", \"detiksulteng\", \"detikmaluku\", \"detjatim\", \"detikcom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita):\n",
    "    s = berita.lower()\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = s.replace('\\r', ' ')\n",
    "    s = s.replace(' o ', ' ')\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'[0-9]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    T = [t for t in tokens if ((t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1790 entries, 0 to 1789\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1790 non-null   object\n",
      " 1   date         1790 non-null   object\n",
      " 2   description  1790 non-null   object\n",
      " 3   source       1790 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.9+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1611\n"
     ]
    }
   ],
   "source": [
    "df_total = pd.read_csv('corpus/dataset/df_total.csv')\n",
    "df_total = df_total[pd.notnull(df_total['description'])]\n",
    "print(df_total.info())\n",
    "print ('-'*90)\n",
    "document_text= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 0 to 178\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        179 non-null    object\n",
      " 1   date         179 non-null    object\n",
      " 2   description  179 non-null    object\n",
      " 3   source       179 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.0+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('corpus/dataset/df_test.csv')\n",
    "df_test = df_test[pd.notnull(df_test['description'])]\n",
    "print(df_test.info())\n",
    "print ('-'*90)\n",
    "document_text_test= joblib.load('corpus/model/desc_text_test.pkl')\n",
    "print(len(document_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1611 entries, 0 to 1610\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1611 non-null   object\n",
      " 1   date         1611 non-null   object\n",
      " 2   description  1611 non-null   object\n",
      " 3   source       1611 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 62.9+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1611\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('corpus/dataset/df_train.csv')\n",
    "df_train = df_train[pd.notnull(df_train['description'])]\n",
    "print(df_train.info())\n",
    "print ('-'*90)\n",
    "document_text_train= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1111111 entries, 0 to 1111110\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count    Dtype \n",
      "---  ------                  --------------    ----- \n",
      " 0   tingkat setelah parent  1111111 non-null  int64 \n",
      " 1   parent                  1111111 non-null  object\n",
      " 2   similarity              1111111 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 25.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_what = pd.read_csv(\"bow/bow_what.csv\")\n",
    "df_bow_what.head()\n",
    "df_bow_what.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1111111 entries, 0 to 1111110\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count    Dtype \n",
      "---  ------                  --------------    ----- \n",
      " 0   tingkat setelah parent  1111111 non-null  int64 \n",
      " 1   parent                  1111051 non-null  object\n",
      " 2   similarity              1111111 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 25.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_when = pd.read_csv(\"bow/bow_when.csv\")\n",
    "df_bow_when.head()\n",
    "df_bow_when.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222222 entries, 0 to 222221\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   tingkat setelah parent  222222 non-null  int64 \n",
      " 1   parent                  222222 non-null  object\n",
      " 2   similarity              222222 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_where = pd.read_csv(\"bow/bow_where.csv\")\n",
    "df_bow_where.head()\n",
    "df_bow_where.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1111111 entries, 0 to 1111110\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count    Dtype \n",
      "---  ------                  --------------    ----- \n",
      " 0   tingkat setelah parent  1111111 non-null  int64 \n",
      " 1   parent                  1111111 non-null  object\n",
      " 2   similarity              1111111 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 25.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_who = pd.read_csv(\"bow/bow_who.csv\")\n",
    "df_bow_who.head()\n",
    "df_bow_who.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'What': 1111111, 'When': 1111111, 'Where': 222222, 'Who': 1111111}\n"
     ]
    }
   ],
   "source": [
    "# Ambil parent dari bow\n",
    "bow_list_what = []\n",
    "bow_list_when = []\n",
    "bow_list_where = []\n",
    "bow_list_who = []\n",
    "\n",
    "for i in range(0, df_bow_what.shape[0]):\n",
    "  bow_list_what.append(df_bow_what.iloc[i, 1])\n",
    "\n",
    "for i in range(0, df_bow_when.shape[0]):\n",
    "  bow_list_when.append(df_bow_when.iloc[i, 1])\n",
    "\n",
    "for i in range(0, df_bow_where.shape[0]):\n",
    "  bow_list_where.append(df_bow_where.iloc[i, 1])\n",
    "\n",
    "for i in range(0, df_bow_who.shape[0]):\n",
    "  bow_list_who.append(df_bow_who.iloc[i, 1])\n",
    "\n",
    "print({\n",
    "  'What': len(bow_list_what),\n",
    "  'When': len(bow_list_when),\n",
    "  'Where': len(bow_list_where),\n",
    "  'Who': len(bow_list_who)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use data train\n",
    "def cari_dokpertama(kueriAsli: str) -> list[str]:\n",
    "    kueriPre = preprocessing(kueriAsli)\n",
    "    kueriPre = \" \".join(kueriPre)\n",
    "    hasilSearch = []\n",
    "    tfidf_matrix = joblib.load('corpus/matrix/tfidf_train.pkl')\n",
    "    tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "    query_vec = tfidf_vectorizer.transform([kueriPre])\n",
    "    results = cosine_similarity(tfidf_matrix, query_vec).reshape((-1))\n",
    "    for i in results.argsort()[-5:][::-1]:\n",
    "        hasilSearch.append(df_total.iloc[i,-2])\n",
    "    hasilSearch=\". \".join(hasilSearch)\n",
    "    return hasilSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Keywords Extraction with YAKE\n",
    "def keyword_yake(hasilSearch: str) -> list[str]:\n",
    "    keywordYake=[]\n",
    "\n",
    "    k_extractor = KeywordExtractor(lan=\"id\", n=1, top=10)\n",
    "    k_extractor2 = KeywordExtractor(lan=\"id\", n=2, top=10)\n",
    "    keywords = k_extractor.extract_keywords(text=hasilSearch)\n",
    "    keywords = k_extractor2.extract_keywords(text=hasilSearch)\n",
    "    keywordYake = [x for x, y in keywords]\n",
    "    #keywordYake.append(keywords)\n",
    "    #print (keywordYake)\n",
    "    return keywordYake\n",
    "#print(\"Keywords of article\\n\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with TFIDF\n",
    "def keyword_tfidf(hasilSearch: str) -> list[str]:\n",
    "\n",
    "    keywordtfidf=[]\n",
    "    keywordtfidf2=[]\n",
    "\n",
    "    total_words = re.sub(r'[^\\w]', ' ', hasilSearch)\n",
    "    total_words = total_words.lower().split()\n",
    "    #print (total_words)\n",
    "    total_word_length = len(total_words)\n",
    "    total_sentences = tokenize.sent_tokenize(hasilSearch)\n",
    "    total_sent_len = len(total_sentences)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    def check_sent(word, sentences): \n",
    "        final = [all([w in x for w in word]) for x in sentences] \n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "        return int(len(sent_len))\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "    #print(idf_score)\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    #print(tf_idf_score)\n",
    "    def get_top_n(dict_elem, n):\n",
    "        result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "        hasil =list(result.keys())\n",
    "        #print(list(result.keys()))        \n",
    "        return hasil\n",
    "    #print(get_top_n(tf_idf_score, 25))\n",
    "    #print(len(get_top_n(tf_idf_score, 1)))\n",
    "    keywordtfidf.append(get_top_n(tf_idf_score, 25))\n",
    "    for i in range(len(keywordtfidf)):\n",
    "        #print (i)\n",
    "        totalKw=0\n",
    "        totalKw=len(keywordtfidf[i])\n",
    "        for j in range(totalKw):\n",
    "            #print (j)\n",
    "            keywordtfidf2.append(keywordtfidf[i][j])\n",
    "    #print (keywordtfidf2)\n",
    "    return keywordtfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with BERT\n",
    "def keyword_bert(hasilSearch: str) -> list[str]:\n",
    "\n",
    "    keywordbert=[]\n",
    "\n",
    "    #for j in range(len(array_text)):\n",
    "    keyword1 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 1))\n",
    "    keyword2 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "    #print(\"Keywords of article\\n\", keywords)\n",
    "    for i in range (0,len (keyword1)):\n",
    "        keywordbert.append(keyword1[i][0])\n",
    "        keywordbert.append(keyword2[i][0])\n",
    "    #print (keywordbert)\n",
    "    return keywordbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rangking (keywordGabung: list[str], kueriAsli: str) -> list[str]:\n",
    "    kandidatFinalCek=[]\n",
    "    kandidatFinalFix=[]\n",
    "    for i in keywordGabung:\n",
    "        if (i not in kandidatFinalCek and i!=0):\n",
    "            kandidatFinalCek.append(i)\n",
    "    queries=[kueriAsli]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(kandidatFinalCek)\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 30\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFinalFix.append(kandidatFinalCek[idx])\n",
    "    print ('kandidatFinalFix: ', kandidatFinalFix)\n",
    "    return kandidatFinalFix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordCustomBow(bowList: list[str], initialQuery: str) -> list[str]:\n",
    "    cekDuplicate = []\n",
    "    kandidatFix = []\n",
    "\n",
    "    for i in bowList:\n",
    "        if(i not in cekDuplicate and i!=0):\n",
    "            cekDuplicate.append(i)\n",
    "\n",
    "    queries=[initialQuery]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(cekDuplicate)\n",
    "    \n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 10\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFix.append(cekDuplicate[idx])\n",
    "    return kandidatFix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kejadian peristiwa berita\n",
      "waktu berita\n",
      "daerah tempat berita\n",
      "orang personel lembaga terlibat berita\n"
     ]
    }
   ],
   "source": [
    "# Creating query for what, when, where, who\n",
    "what_initial_query = \"apa sebenarnya kejadian atau peristiwa dalam berita\"\n",
    "when_initial_query = \"kapan waktu berita tersebut terjadi\"\n",
    "where_initial_query = \"di daerah mana tempat berita itu terjadi\"\n",
    "who_initial_query = \"siapa orang, personel, atau lembaga yang terlibat dalam berita\"\n",
    "\n",
    "what_query = preprocessing(what_initial_query)\n",
    "what_query = \" \".join(what_query)\n",
    "print (what_query)\n",
    "\n",
    "when_query = preprocessing(when_initial_query)\n",
    "when_query = \" \".join(when_query)\n",
    "print (when_query)\n",
    "\n",
    "where_query = preprocessing(where_initial_query)\n",
    "where_query = \" \".join(where_query)\n",
    "print (where_query)\n",
    "\n",
    "who_query = preprocessing(who_initial_query)\n",
    "who_query = \" \".join(who_query)\n",
    "print (who_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepareWData(initial_query: str, bow_list: list[str]):\n",
    "    hasilkandidat = []\n",
    "    keywordGabung = []\n",
    "    kandidatFix = []\n",
    "    kueriFix = []\n",
    "\n",
    "    hasilSearch     = cari_dokpertama(initial_query)\n",
    "    keywordYake     = keyword_yake(hasilSearch)\n",
    "    keywordtfidf2   = keyword_tfidf(hasilSearch)\n",
    "    keywordbert     = keyword_bert(hasilSearch)\n",
    "    keywordBoW      = keywordCustomBow(bow_list, initial_query)\n",
    "\n",
    "    for keyword in keywordYake:\n",
    "        keywordGabung.append(keyword)\n",
    "    for keyword in keywordtfidf2:\n",
    "        keywordGabung.append(keyword)\n",
    "    for keyword in keywordbert:\n",
    "        keywordGabung.append(keyword)    \n",
    "    for keyword in keywordBoW:\n",
    "        keywordGabung.append(keyword)\n",
    "\n",
    "    hasilrank = rangking(keywordGabung, initial_query)\n",
    "    \n",
    "    for word in hasilrank:\n",
    "        kueriFix.append(word)\n",
    "    for word in kueriFix:\n",
    "        hasilkandidat.append(word)\n",
    "\n",
    "    kueriFix = [preprocessing(i) for i in kueriFix]\n",
    "    \n",
    "    for word in kueriFix:\n",
    "        for subWord in word:\n",
    "            kandidatFix.append(subWord)\n",
    "\n",
    "    kandidatFix = [\" \".join(kandidatFix)]\n",
    "    # print('*'*120)\n",
    "    return kandidatFix, hasilrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kandidatFinalFix:  ['stiria', 'noticias', 'satawan', 'sastranegara', 'mondulkiri', 'daerahnya', 'masayarakat', 'daerah', 'krajeńskie', 'kawasan', 'peristiwa', 'darurat', 'penguatan', 'kabupaten', 'kecamatan', 'rumah warga', 'personel', 'kebencanaan', 'melaporkan', 'pulusangi', 'tenggara', 'penguatan mewaspadai', 'mencatat', 'warga', 'rumah', 'merusak', 'terdekat', 'menggambarkan', 'pulusangi kecamatan', 'sulawesi']\n",
      "kandidatFinalFix:  ['informations', 'peritiwa', 'kejadiannya', 'tajuknya', 'memancarkan', 'tinjauan', 'peristiwa', 'mengobarkan', 'kejadian', 'premis', 'pemangkasan', 'darurat', 'penguatan', 'penguatan mewaspadai', 'merusak', 'pulusangi', 'mengakibatkan', 'melaporkan', 'penanganan', 'kencang merusak', 'pembangunan', 'kencang menyikapi', 'bersamaan', 'kapusdatinkom', 'kabupaten', 'aktivitas', 'personel', 'kecamatan', 'prakiraan cuaca', 'terdekat']\n",
      "kandidatFinalFix:  ['wiraswastawan', 'wiracaritawan', 'pelaporan', 'investigatif', 'wartawan', 'personelnya', 'hartawan', 'investigative', 'stafnya', 'staffnya', 'informasikan', 'department', 'operation', 'keterangan', 'pemeriksaan', 'manajemen', 'store', 'buka', 'tertulisnya', 'department store', 'kabupaten', 'mengalami', 'perbaikan', 'kecamatan', 'sentul', 'ruangan tertutup', 'ruang panel', 'kekuatan', 'mall operation', 'pagi']\n",
      "kandidatFinalFix:  ['jurnalistiknya', 'akhbar', 'pelaporan', 'zamannya', 'zaman', 'ajalnya', 'ajal', 'waktunya', 'masanya', 'ajalku', 'peristiwa', 'puskesmas', 'cakupan', 'darurat', 'kebencanaan', 'pulusangi', 'bersamaan', 'memfasilitasi', 'merusak', 'personel', 'menggambarkan', 'penguatan mewaspadai', 'lempeng', 'melaporkan', 'kabupaten', 'kecamatan', 'batas lempeng', 'rumah', 'kapusdatinkom', 'rumah warga']\n"
     ]
    }
   ],
   "source": [
    "where_keyword_candidates, where_rank_result = prepareWData(where_query, bow_list_where)\n",
    "what_keyword_candidates, what_rank_result = prepareWData(what_query, bow_list_what)\n",
    "who_keyword_candidates, who_rank_result = prepareWData(who_query, bow_list_who)\n",
    "when_keyword_candidates, when_rank_result = prepareWData(when_query, bow_list_when)\n",
    "\n",
    "def wCalculation():\n",
    "    document_result = []\n",
    "\n",
    "    document_result_what = []\n",
    "    document_result_who = []\n",
    "    document_result_when = []\n",
    "    document_result_where = []\n",
    "\n",
    "    for i in range(0, len(document_text_test)-1):\n",
    "        hasilWhat = []\n",
    "        hasilWho = []\n",
    "        hasilWhen = []\n",
    "        hasilWhere = []\n",
    "\n",
    "        teks = df_total.iloc[i, -2]\n",
    "        tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "        query_vec_what = tfidf_vectorizer.transform(what_keyword_candidates)\n",
    "        results_what = cosine_similarity(tfidf_matrix, query_vec_what).reshape((-1))\n",
    "        document_result_what.append(df_total.iloc[i, 2])\n",
    "\n",
    "        for a in what_rank_result:\n",
    "            cariW = re.findall(a, document_result_what[i])\n",
    "            if cariW:\n",
    "                hasilWhat.append(a)\n",
    "\n",
    "        document_result.append([i, 'what', what_keyword_candidates, hasilWhat, results_what, 0, 0])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_who = tfidf_vectorizer.transform(who_keyword_candidates)\n",
    "        results_who = cosine_similarity(tfidf_matrix, query_vec_who).reshape((-1))\n",
    "        document_result_who.append(df_total.iloc[i, 2])\n",
    "\n",
    "        for a in who_rank_result:\n",
    "            cariW = re.findall(a, document_result_who[i])\n",
    "            if cariW:\n",
    "                hasilWho.append(a)\n",
    "\n",
    "        document_result.append([i, 'who', who_keyword_candidates, hasilWho, results_who, 0, 0])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_when = tfidf_vectorizer.transform(when_keyword_candidates)\n",
    "        results_when = cosine_similarity(tfidf_matrix, query_vec_when).reshape((-1))\n",
    "        document_result_when.append(df_total.iloc[i, 2])\n",
    "\n",
    "        for a in when_rank_result:\n",
    "            cariW = re.findall(a, document_result_when[i])\n",
    "            if cariW:\n",
    "                hasilWhen.append(a)\n",
    "\n",
    "        document_result.append([i, 'when', when_keyword_candidates, hasilWhen, results_when, 0, 0])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_where = tfidf_vectorizer.transform(where_keyword_candidates)\n",
    "        results_where = cosine_similarity(tfidf_matrix, query_vec_where).reshape((-1))\n",
    "        document_result_where.append(df_total.iloc[i, 2])\n",
    "\n",
    "        for a in where_rank_result:\n",
    "            cariW = re.findall(a, document_result_where[i])\n",
    "            if cariW:\n",
    "                hasilWhere.append(a)\n",
    "\n",
    "        document_result.append([i, 'where', where_keyword_candidates, hasilWhere, results_where, 0, 0])\n",
    "    \n",
    "    writer = pd.DataFrame(document_result, columns=['Data','W', 'Query', 'Query Result', 'Similarity', 'True Positive', 'True Negative'])\n",
    "    writer.to_csv('QE_Stat_V2_testing_result.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wCalculation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
