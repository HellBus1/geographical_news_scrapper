{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\QE Statistik\\QE Statistik.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/QE%20Statistik/QE%20Statistik.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/QE%20Statistik/QE%20Statistik.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeybert\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyBERT\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/QE%20Statistik/QE%20Statistik.ipynb#ch0000000?line=14'>15</a>\u001b[0m kw_extractor \u001b[39m=\u001b[39m KeyBERT(\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-nli-mean-tokens\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/QE%20Statistik/QE%20Statistik.ipynb#ch0000000?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/QE%20Statistik/QE%20Statistik.ipynb#ch0000000?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\keybert\\_model.py:49\u001b[0m, in \u001b[0;36mKeyBERT.__init__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=33'>34</a>\u001b[0m              model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=34'>35</a>\u001b[0m     \u001b[39m\"\"\" KeyBERT initialization\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=35'>36</a>\u001b[0m \n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=36'>37</a>\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=46'>47</a>\u001b[0m \u001b[39m                  * https://www.sbert.net/docs/pretrained_models.html\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=47'>48</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/_model.py?line=48'>49</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m select_backend(model)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\keybert\\backend\\_utils.py:43\u001b[0m, in \u001b[0;36mselect_backend\u001b[1;34m(embedding_model)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_utils.py?line=40'>41</a>\u001b[0m \u001b[39m# Create a Sentence Transformer model based on a string\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_utils.py?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding_model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_utils.py?line=42'>43</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SentenceTransformerBackend(embedding_model)\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_utils.py?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m SentenceTransformerBackend(\u001b[39m\"\u001b[39m\u001b[39mparaphrase-multilingual-MiniLM-L12-v2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py:35\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.__init__\u001b[1;34m(self, embedding_model)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=32'>33</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m embedding_model\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=33'>34</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding_model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=34'>35</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m SentenceTransformer(embedding_model)\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=35'>36</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=36'>37</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease select a correct SentenceTransformers model: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=37'>38</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m`from sentence_transformers import SentenceTransformer` \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/keybert/backend/_sentencetransformers.py?line=38'>39</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m`model = SentenceTransformer(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)`\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:86\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=82'>83</a>\u001b[0m     model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=84'>85</a>\u001b[0m     \u001b[39m# Download from hub with caching\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=85'>86</a>\u001b[0m     snapshot_download(model_name_or_path,\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=86'>87</a>\u001b[0m                         cache_dir\u001b[39m=\u001b[39;49mcache_folder,\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=87'>88</a>\u001b[0m                         library_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msentence-transformers\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=88'>89</a>\u001b[0m                         library_version\u001b[39m=\u001b[39;49m__version__,\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=89'>90</a>\u001b[0m                         ignore_files\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mflax_model.msgpack\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrust_model.ot\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtf_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=90'>91</a>\u001b[0m                         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token)\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/SentenceTransformer.py?line=93'>94</a>\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\sentence_transformers\\util.py:458\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=452'>453</a>\u001b[0m nested_dirname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=453'>454</a>\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(storage_folder, relative_filepath)\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=454'>455</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=455'>456</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(nested_dirname, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=457'>458</a>\u001b[0m path \u001b[39m=\u001b[39m cached_download(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=458'>459</a>\u001b[0m     url,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=459'>460</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mstorage_folder,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=460'>461</a>\u001b[0m     force_filename\u001b[39m=\u001b[39;49mrelative_filepath,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=461'>462</a>\u001b[0m     library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=462'>463</a>\u001b[0m     library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=463'>464</a>\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=464'>465</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=465'>466</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=467'>468</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/sentence_transformers/util.py?line=468'>469</a>\u001b[0m     os\u001b[39m.\u001b[39mremove(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:344\u001b[0m, in \u001b[0;36mcached_download\u001b[1;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=341'>342</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m local_files_only:\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=342'>343</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=343'>344</a>\u001b[0m         r \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=344'>345</a>\u001b[0m             method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=345'>346</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=346'>347</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=347'>348</a>\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=348'>349</a>\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=349'>350</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=350'>351</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=351'>352</a>\u001b[0m         r\u001b[39m.\u001b[39mraise_for_status()\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=352'>353</a>\u001b[0m         etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:232\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=229'>230</a>\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=230'>231</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=231'>232</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=232'>233</a>\u001b[0m         method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=233'>234</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=234'>235</a>\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/huggingface_hub/file_download.py?line=235'>236</a>\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/api.py?line=56'>57</a>\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/api.py?line=57'>58</a>\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/api.py?line=58'>59</a>\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=523'>524</a>\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=524'>525</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=525'>526</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=526'>527</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=641'>642</a>\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/sessions.py?line=647'>648</a>\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=437'>438</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/requests/adapters.py?line=454'>455</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=699'>700</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=716'>717</a>\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=383'>384</a>\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=384'>385</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=385'>386</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=386'>387</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=387'>388</a>\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=388'>389</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1037'>1038</a>\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1039'>1040</a>\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1041'>1042</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1042'>1043</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1043'>1044</a>\u001b[0m         (\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1044'>1045</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1049'>1050</a>\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connectionpool.py?line=1050'>1051</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=355'>356</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=356'>357</a>\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=357'>358</a>\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=358'>359</a>\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=359'>360</a>\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=170'>171</a>\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=172'>173</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=173'>174</a>\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=174'>175</a>\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=175'>176</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=177'>178</a>\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=178'>179</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=179'>180</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=180'>181</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=181'>182</a>\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[0;32m    <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/connection.py?line=182'>183</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\HERRI\\Tugas Akhir\\pra\\geographical_news_scrapper\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/util/connection.py?line=82'>83</a>\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/util/connection.py?line=83'>84</a>\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/util/connection.py?line=84'>85</a>\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/util/connection.py?line=85'>86</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[0;32m     <a href='file:///d%3A/HERRI/Tugas%20Akhir/pra/geographical_news_scrapper/venv/lib/site-packages/urllib3/util/connection.py?line=87'>88</a>\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import library-library\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation and Preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "# Word Embedding\n",
    "import joblib\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "\n",
    "# Input and Expansion Query\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from yake import KeywordExtractor\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "NLTK_StopWords = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita):\n",
    "    s = berita.lower()\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = s.replace('\\r', ' ')\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    T = [t for t in tokens if (((t.lower() == \"tempat\") or (t.lower() == \"waktu\") or (t.lower() == \"hari\")) or (t not in NLTK_StopWords))]\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['olahraga', 'fengan', 'sukamu']\n"
     ]
    }
   ],
   "source": [
    "kta=\"olahraga fengan sukamu\"\n",
    "print(preprocessing(kta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 0 to 157\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        158 non-null    object\n",
      " 1   date         158 non-null    object\n",
      " 2   description  158 non-null    object\n",
      " 3   source       158 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 6.2+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_test = df_test[pd.notnull(df_test['description'])]\n",
    "print(df_test.info())\n",
    "print ('-'*90)\n",
    "document_text_test= joblib.load('document_text_test.pkl')\n",
    "print(len(document_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1418 entries, 0 to 1418\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1418 non-null   object\n",
      " 1   date         1418 non-null   object\n",
      " 2   description  1418 non-null   object\n",
      " 3   source       1418 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 55.4+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1419\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train = df_train[pd.notnull(df_train['description'])]\n",
    "print(df_train.info())\n",
    "print ('-'*90)\n",
    "document_text_train= joblib.load('document_text_train.pkl')\n",
    "print(len(document_text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_read(bow):\n",
    "    bow_read = pd.read_csv(bow)\n",
    "    bow_text = []\n",
    "\n",
    "    for i in range(0,bow_read.shape[0]):\n",
    "        if(bow_read.iloc[i,1] not in bow_text and bow_read.iloc[i,1]!=0):\n",
    "            bow_text.append(bow_read.iloc[i,1])\n",
    "        \n",
    "    return(bow_read,bow_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cari_dokpertama(kueriAsli):\n",
    "    kueriPre=preprocessing(kueriAsli)\n",
    "    kueriPre= \" \".join (kueriPre)\n",
    "    hasilSearch=[]\n",
    "    tfidf_train = joblib.load('tfidf_train.pkl')\n",
    "    tfidf_vectorizer = joblib.load('vectorizer.pkl')\n",
    "    query_vec= tfidf_vectorizer.transform([kueriPre])\n",
    "    # print('queryvec')\n",
    "    print(query_vec)\n",
    "    results=cosine_similarity(tfidf_train, query_vec).reshape((-1))\n",
    "    # print(results)\n",
    "    for i in results.argsort()[-5:][::-1]:\n",
    "        hasilSearch.append(df_train.iloc[i,-2])\n",
    "    hasilSearch=\". \".join(hasilSearch)\n",
    "    return hasilSearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Keywords Extraction with YAKE\n",
    "def keyword_yake(hasilSearch):\n",
    "    # print(hasilSearch)\n",
    "    keywordYake=[]\n",
    "\n",
    "    k_extractor = KeywordExtractor(lan=\"id\", n=1, top=50)\n",
    "    # print(k_extractor)\n",
    "    k_extractor2 = KeywordExtractor(lan=\"id\", n=2, top=50)\n",
    "    # print(k_extractor2)\n",
    "    keywords = k_extractor.extract_keywords(text=hasilSearch)\n",
    "    # print(\"pertama : \",keywords)\n",
    "    keywords = k_extractor2.extract_keywords(text=hasilSearch)\n",
    "    # print(\"kedua : \",keywords)\n",
    "    keywordYake = [x for x, y in keywords]\n",
    "    # print(keywordYake)\n",
    "    #keywordYake.append(keywords)\n",
    "    # print (keywordYake)\n",
    "    return keywordYake\n",
    "#print(\"Keywords of article\\n\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with TFIDF\n",
    "def keyword_tfidf(hasilSearch):\n",
    "\n",
    "    keywordtfidf=[]\n",
    "    keywordtfidf2=[]\n",
    "\n",
    "    #doc = 'بَاب فرض الْوضُوء وسننه وهيآته وَفرض الْوضُوء سِتّ خِصَال النِّيَّة عمند غسل الْوَجْه وَغسل الْوَجْه وَغسل الذراعين مَعَ الْمرْفقين وَمسح مَا قل من الرَّأْس وَغسل الرجلَيْن مَعَ الْكَعْبَيْنِ وَالتَّرْتِيب وعَلى قَول الْوَلَاء وسننه عشر خِصَال خمس مِنْهَا قبل غسل الْوَجْه وَهِي التَّسْمِيَة وَغسل الْكَفَّيْنِ والمضمضة وَالِاسْتِنْشَاق وَالْمُبَالغَة فيههما إِلَّا للصَّائِم وَخمْس بعد غسل الْوَجْه وَهِي تَقْدِيم الْيُمْنَى على ليسرى وَمسح جَمِيع الرَّأْس وَمسح الْأُذُنَيْنِ ظاهرهما وباطنهما وَإِدْخَال الأصبعين فيهمَا وتخليل أَصَابِع الرجلَيْن . وَغسل دَاخل الْكَعْبَيْنِ وَلَيْسَ مسح لعنق من سنَنه وفضيلته تكراره ثَلَاثًا وزالواجب فِيهِ مرّة والمرتان أفضل وَالثَّلَاث أكمل وهيآته أَن يبْدَأ فِي تَطْهِير الْأَعْضَاء بمواضع الِابْتِدَاء . فَإِن اقْتصر على فروضه استة أَجزَأَهُ وَإِن ضيع حَظّ نَفسه فِيمَا ترك'\n",
    "    total_words = re.sub(r'[^\\w]', ' ', hasilSearch)\n",
    "    total_words = total_words.lower().split()\n",
    "    #print (total_words)\n",
    "    total_word_length = len(total_words)\n",
    "    total_sentences = tokenize.sent_tokenize(hasilSearch)\n",
    "    total_sent_len = len(total_sentences)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in NLTK_StopWords:\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    def check_sent(word, sentences): \n",
    "        final = [all([w in x for w in word]) for x in sentences] \n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "        return int(len(sent_len))\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in NLTK_StopWords:\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "    #print(idf_score)\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    #print(tf_idf_score)\n",
    "    def get_top_n(dict_elem, n):\n",
    "        result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "        hasil =list(result.keys())\n",
    "        #print(list(result.keys()))        \n",
    "        return hasil\n",
    "    #print(get_top_n(tf_idf_score, 25))\n",
    "    #print(len(get_top_n(tf_idf_score, 1)))\n",
    "    keywordtfidf.append(get_top_n(tf_idf_score, 35))\n",
    "    for i in range(len(keywordtfidf)):\n",
    "        #print (i)\n",
    "        totalKw=0\n",
    "        totalKw=len(keywordtfidf[i])\n",
    "        for j in range(totalKw):\n",
    "            #print (j)\n",
    "            keywordtfidf2.append(keywordtfidf[i][j])\n",
    "    # print (keywordtfidf2)\n",
    "    return keywordtfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with BERT\n",
    "def keyword_bert(hasilSearch):\n",
    "    # print(hasilSearch)\n",
    "\n",
    "    keywordbert=[]\n",
    "\n",
    "    #for j in range(len(array_text)):\n",
    "    keyword1 = kw_extractor.extract_keywords(hasilSearch, top_n=50, keyphrase_ngram_range=(1, 1))\n",
    "    # print(keyword1)\n",
    "    keyword2 = kw_extractor.extract_keywords(hasilSearch, top_n=50, keyphrase_ngram_range=(1, 2))\n",
    "    # print(keyword2)\n",
    "    # keyword3 = kw_extractor.extract_keywords(hasilSearch, top_n=20, keyphrase_ngram_range=(1, 4))\n",
    "    # print(keyword3)\n",
    "\n",
    "    #print(\"Keywords of article\\n\", keywords)\n",
    "    for i in range (0,len (keyword1)):\n",
    "        keywordbert.append(keyword1[i][0])\n",
    "        keywordbert.append(keyword2[i][0])\n",
    "    # print (keywordbert)\n",
    "    # print(len(keywordbert))\n",
    "    return keywordbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('medan', 0.5339), ('pembacokan', 0.5201), ('tangerang', 0.5142), ('bersangkutan', 0.5107), ('gabungan', 0.497), ('penembakan', 0.4725), ('cengkareng', 0.4716), ('pembunuhan', 0.4686), ('jakarta', 0.4684), ('mengamankan', 0.4634), ('mengakibatkan', 0.4623), ('penganiayaan', 0.4603), ('pelaku', 0.4602), ('pemeriksaan', 0.4594), ('perumahan', 0.4553), ('langsung', 0.4528), ('kelompok', 0.4503), ('anggota', 0.4492), ('green', 0.4479), ('peristiwa', 0.4478), ('tubagus', 0.4473), ('perusakan', 0.4457), ('penyelidikan', 0.4456), ('bacokan', 0.4372), ('pertigaan', 0.4332), ('berboncengan', 0.4266), ('rangkaian', 0.4236), ('mendiskusikan', 0.4215), ('mengalami', 0.4151), ('dirkrimum', 0.415), ('orang', 0.4148), ('kosambi', 0.414), ('minggu', 0.4102), ('pimpinan', 0.4048), ('lake', 0.4042), ('polda', 0.4037), ('tangannya', 0.4013), ('bacok', 0.4004), ('kriminal', 0.3976), ('diduga', 0.3959), ('korban', 0.3935), ('kapolda', 0.393), ('kombes', 0.3907), ('metro', 0.3899), ('meninggal', 0.3894), ('dunia', 0.3888), ('tangan', 0.3833), ('darurat', 0.3829), ('kota', 0.3818), ('satgas', 0.3777)]\n",
      "[('perumahan green', 0.6167), ('pembacokan mengakibatkan', 0.5977), ('penembakan perumahan', 0.5951), ('tangerang rangkaian', 0.5887), ('polres tangerang', 0.5851), ('insiden pembacokan', 0.5818), ('cengkareng jakarta', 0.5794), ('pembacokan duri', 0.5791), ('medan satria', 0.5786), ('kosambi cengkareng', 0.5755), ('pembunuhan perusakan', 0.5746), ('pembacokan pertigaan', 0.5698), ('penganiayaan pembunuhan', 0.5676), ('gabungan polres', 0.5646), ('rangkaian peristiwa', 0.5641), ('diawali pembacokan', 0.5615), ('kombes tubagus', 0.5605), ('kriminal langsung', 0.5595), ('tangerang kota', 0.5591), ('mengakibatkan orang', 0.558), ('perusakan undang', 0.5549), ('er pembacokan', 0.5539), ('pelaku penganiayaan', 0.5536), ('bersangkutan meninggal', 0.5535), ('green lake', 0.5493), ('er bersangkutan', 0.5483), ('city tangerang', 0.5472), ('markasnya medan', 0.5451), ('polisi mengamankan', 0.5443), ('orang kelompok', 0.5428), ('diduga pelaku', 0.5356), ('kota anggota', 0.5347), ('rangkaian minggu', 0.5343), ('medan', 0.5339), ('penyelidikan diduga', 0.5322), ('langsung direktur', 0.5273), ('orang meninggal', 0.5252), ('dirkrimum polda', 0.5247), ('peristiwa rangkaian', 0.5239), ('bacokan nama', 0.523), ('khusus gabungan', 0.5229), ('pimpinan dirkrimum', 0.5229), ('jakarta barat', 0.5222), ('diduga kelompok', 0.5217), ('kejadian peristiwa', 0.5208), ('pembacokan', 0.5201), ('jakarta senin', 0.5186), ('jaya kombes', 0.5183), ('diadang pelaku', 0.517), ('tangerang', 0.5142)]\n",
      "[('green lake city tangerang', 0.7644), ('perumahan green lake city', 0.7388), ('barat penembakan perumahan green', 0.7355), ('jakarta barat penembakan perumahan', 0.7066), ('penembakan perumahan green lake', 0.7066), ('penembakan perumahan green', 0.7062), ('pelaku penganiayaan pembunuhan perusakan', 0.6949), ('penganiayaan pembunuhan perusakan undang', 0.6945), ('metro jaya kombes tubagus', 0.6905), ('metro jaya insiden pembacokan', 0.6896), ('insiden pembacokan duri kosambi', 0.6874), ('lake city tangerang rangkaian', 0.6838), ('jaya insiden pembacokan duri', 0.6754), ('langsung direktur kombes tubagus', 0.6749), ('pembacokan duri kosambi cengkareng', 0.673), ('polres tangerang kota anggota', 0.6681), ('penganiayaan pembunuhan perusakan', 0.6661), ('tangerang rangkaian peristiwa kejadian', 0.6659), ('dirkrimum polda metro jaya', 0.6657), ('cengkareng jakarta barat minggu', 0.6654)]\n"
     ]
    }
   ],
   "source": [
    "hasilDokumenWhen=[]\n",
    "# kueriAsliWhen='hari apa waktu terjadinya'\n",
    "hasilSearch=\"polda metro jaya insiden pembacokan duri kosambi cengkareng jakarta barat penembakan perumahan green lake city tangerang rangkaian peristiwa kejadian polisi mengamankan 25 orang john kei markasnya medan satria bekasi kapolda metro jaya irjen nana sudjana timnya pimpinan dirkrimum polda metro jaya kombes tubagus ade hidayat olah tempat kejadian peristiwa tkp pemeriksaan saksi saksi tim khusus satgas antibegal preman tim khusus gabungan polres tangerang kota anggota direktorat reserse kriminal langsung direktur kombes tubagus ade mendiskusikan penyelidikan hasil penyelidikan diduga pelaku penganiayaan pembunuhan perusakan undang undang darurat uu nomor 12 1951 irjen nana jumpa pers mapolda metro jaya jakarta senin 22 6 2020 nana peristiwa rangkaian minggu 21 6 kejadian diawali pembacokan mengakibatkan orang tewas ydr alias er pembacokan pertigaan abc duri kosambi cengkareng jakarta barat minggu 21 6 11 30 wib korban er ar berboncengan motor diadang pelaku turun mobil diduga kelompok john kei 5 7 orang kelompok nus kei wilayah kosambi cengkareng jakarta barat menyebabkan 1 orang meninggal dunia nama er bersangkutan meninggal luka bacok tempat irjen nana korban berinisial ar mengalami luka bacok orang putus jari tangannya 4 jari tangan putus bacokan nama ar\"\n",
    "keywordber=keyword_bert(hasilSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rangking (keywordGabung):\n",
    "    keywordTemp=[]\n",
    "    keywordFinal=[]\n",
    "\n",
    "    def borda_sort(lists):\n",
    "        scores = {}\n",
    "        for l in lists:\n",
    "            for idx, elem in enumerate(reversed(l)):\n",
    "                if not elem in scores:\n",
    "                    scores[elem] = 0\n",
    "                scores[elem] += idx\n",
    "        return sorted(scores.keys(), key=lambda elem: scores[elem], reverse=True)\n",
    "\n",
    "    keywordTemp.append(borda_sort(keywordGabung))\n",
    "    print ('kandidat temp',keywordTemp)\n",
    "    print ('Total Kandidat temp: ',len(keywordTemp[0]))\n",
    "\n",
    "    if len(keywordTemp[0])>30:\n",
    "        print ('kurang dari 80')\n",
    "        for i in range (0,80):\n",
    "            keywordFinal.append(keywordTemp[0][i])\n",
    "    elif len(keywordTemp[0])<80:\n",
    "        print ('lebih dari 80')\n",
    "        for i in range (0,len(keywordTemp)):\n",
    "            for j in range (0,len(keywordTemp[0])):\n",
    "                keywordFinal.append(keywordTemp[0][j])\n",
    "    print ('Total Kandidat final: ',len(keywordFinal))\n",
    "    print ('Kandidat final: ',keywordFinal)\n",
    "    return keywordFinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_BOW(keywordBOW, kueriAsli):\n",
    "    cekDuplicate = []\n",
    "    kandidatFix = []\n",
    "\n",
    "    for i in keywordBOW:\n",
    "        if(i not in cekDuplicate and i!=0):\n",
    "            cekDuplicate.append(i)\n",
    "\n",
    "    # queries=[kueriAsli]\n",
    "    queries=kueriAsli\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(cekDuplicate)\n",
    "    \n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 1000\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFix.append(cekDuplicate[idx])\n",
    "    return kandidatFix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kandidatFix(kueriAsli,bow):\n",
    "    #kueri what\n",
    "    # kueriAsli='apa sebenarnya kejadian yang terjadi diberita tersebut'\n",
    "    kueri=preprocessing(kueriAsli)\n",
    "    kueri= [\" \".join (kueri)]\n",
    "    print (kueri)\n",
    "    hasilkandidat=[]\n",
    "    keywordGabung=[]\n",
    "    kandidatFix=[]\n",
    "    kueriFix=[]\n",
    "    kueriFixWithDelimiter=[]\n",
    "    \n",
    "\n",
    "\n",
    "    hasilSearch=cari_dokpertama(kueriAsli)\n",
    "    keywordYake=keyword_yake(hasilSearch)\n",
    "    keywordtfidf2=keyword_tfidf(hasilSearch)\n",
    "    keywordbert=keyword_bert (hasilSearch)\n",
    "    keywordBOW=keyword_BOW(bow, kueri)\n",
    "\n",
    "    keywordGabung.append(keywordYake)\n",
    "    keywordGabung.append(keywordtfidf2)\n",
    "    keywordGabung.append(keywordbert)\n",
    "    # keywordGabung.append(keywordBOW)\n",
    "    hasilrank=rangking(keywordGabung)\n",
    "\n",
    "    for i in hasilrank:\n",
    "        kueriFix.append(i)\n",
    "    for x in keywordBOW:\n",
    "        kueriFix.append(x)\n",
    "    for j in kueriFix:\n",
    "        hasilkandidat.append(j)\n",
    "    kueriFixWithDelimiter=kueriFix\n",
    "    kueriFix=[preprocessing(i) for i in kueriFix]\n",
    "    for i in kueriFix:\n",
    "        for j in i:\n",
    "            kandidatFix.append(j)\n",
    "    kandidatFix= [\" \".join (kandidatFix)]\n",
    "    \n",
    "    print ('*'*120)\n",
    "    return(kandidatFix,keywordGabung,keywordBOW,hasilrank,kueriFixWithDelimiter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_kecelakaan_read,bow_kecelakaan_text = bow_read('bow_kecelakaan.csv')\n",
    "bow_where_read,bow_where_text = bow_read('bow_where.csv')\n",
    "bow_when_read,bow_when_text = bow_read('bow_when.csv')\n",
    "bow_who_read,bow_who_text = bow_read('bow_who.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What     :  musibah\n",
      "Where    :  semarang\n",
      "When     :  waktunya\n",
      "Who      :  pelakunya\n"
     ]
    }
   ],
   "source": [
    "print(\"What     : \", bow_kecelakaan_text[0])\n",
    "print(\"Where    : \", bow_where_text[0])\n",
    "print(\"When     : \", bow_when_text[0])\n",
    "print(\"Who      : \", bow_who_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11110\n",
      "5576\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_when_text))\n",
    "cekDuplicate1 = []\n",
    "kandidatFix2 = []\n",
    "\n",
    "for i in bow_kecelakaan_text:\n",
    "    if(i not in cekDuplicate1 and i!=0):\n",
    "        cekDuplicate1.append(i)\n",
    "print(len(cekDuplicate1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pelaku kejadian']\n",
      "  (0, 10695)\t0.8756401519728193\n",
      "  (0, 6678)\t0.4829641024476021\n",
      "['kereta api', 'perlintasan sebidang', 'kereta', 'palang pintu', 'perlintasan', 'api', 'perlintasan kereta', 'sebidang', 'pengguna jalan', 'api perlintasan', 'jalan', 'keselamatan perlintasan', 'angkutan jalan', 'rel kereta', 'sebidang sesuai', 'kecelakaan perlintasan', 'lintas angkutan', 'kecelakaan', 'mobil', 'api jalan', 'palang', 'pintu kereta', 'polres kediri', 'lintas perlintasan', 'mobil dikendarai', 'blitar kediri', 'kediri akp', 'kediri penyelidikan', 'api peristiwa', 'lintas', 'pintu', 'melintas perlintasan', 'dikendarai ongki', 'rusak parah', 'jalan raya', 'api sebidang', 'api rapih', 'api palang', 'tol', 'berbunyi palang', 'kediri kecelakaan', 'sebidang kereta', 'mengalami luka', 'perjalanan kereta', 'munculah kereta', 'ongki agus', 'warga sedati', 'arah barat', 'pengguna', 'kendaraan']\n",
      "['17', 'kecelakaan', '2009', 'confero', 'bernopol', 'n', '1835', 'ga', 'ditumpangi', 'terseret', '50', 'meter', 'terbalik', 'sisi', 'ongki', 'agus', '41', 'warga', 'sedati', 'sidoarjo', 'sang', 'langsung', 'menerobosnya', 'munculah', 'rapih', 'dhoho', 'blitar', 'dihindarkan', 'akp', 'bobby', 'zulfikar', 'oukul', '15', '00', 'berat']\n",
      "['50', 'terseret 50', '45', '50 meter', 'cc2039814', '45 wib', 'benz', 'cc2039814 dimasinisi', '396', '41 warga', '1835', 'log cc2039814', 'munculah', 'benz menyebabkan', 'keluarga', '8438 dikemudikan', '41', 'ka 396', 'mengedepankan', 'bernopol 1835', 'rapih', '2501 melintas', 'kereta', 'mobil keluarga', 'kemacetan', 'agus 41', 'kepedulian', 'mengedepankan kedepankan', 'menerobosnya', 'langsung menerobosnya', 'didahulukan', '50', 'terluka', 'mendahulukan kereta', 'pemerintah', 'pintu kereta', 'mengungkap', 'munculah kereta', 'kedepankan', 'kemacetan pengguna', 'mendahulukan', '45', 'tetapkan', '000 keselamatan', 'mengemudikan', 'pengendara toyota', 'mengancam', 'rp 750', 'mengimbau', '11 45', 'angkutan', 'kecelakaan ditetapkan', 'dikemudikan', '1835 ga', 'berbunyi', 'tertutup kereta', 'langsung', 'diharapkan kepedulian', 'musyawarah', 'mengancam keselamatan', 'ditetapkan', '396 nolog', 'kemarin', 'ditetapkan tersangka', 'menciptakan', 'joni kemacetan', 'dihubungi', '114 peraturan', 'kecelakaan', 'truk colt', '2501', 'cc2039814', '750', 'keluarga abg', 'mengakibatkan', 'tabrakan kereta', 'mengaku', 'kedepankan musyawarah', 'juta', 'mobil mercedes', 'kerusakan', 'lokomotif kereta', 'dievakuasi', 'truk dievakuasi', 'tertabrak', 'bersamaan munculah', 'melibatkan', 'menyebabkan kecelakaan', 'bachtiar', 'pasal 114', 'mengajak', 'mengimbau pengguna', 'bunyi', 'pintu dibuka', 'mercedes', 'kereta rusak', 'meninggalkan', 'menciptakan keselamatan', 'tersangka', '30 wib']\n",
      "kandidat temp [['50', '45', 'kereta', 'cc2039814', '1835', '41', 'munculah', 'terseret 50', 'kecelakaan', '50 meter', '45 wib', 'benz', 'pintu kereta', 'cc2039814 dimasinisi', '396', '41 warga', 'rapih', 'log cc2039814', 'benz menyebabkan', 'keluarga', '8438 dikemudikan', 'menerobosnya', 'ka 396', 'mengedepankan', 'bernopol 1835', '2501 melintas', 'mobil keluarga', 'kemacetan', 'agus 41', 'kepedulian', 'mengedepankan kedepankan', 'langsung menerobosnya', 'didahulukan', 'munculah kereta', 'terluka', 'mendahulukan kereta', 'pemerintah', 'mengungkap', 'kedepankan', 'kemacetan pengguna', 'mendahulukan', 'tetapkan', 'langsung', '000 keselamatan', 'mengemudikan', 'pengendara toyota', 'mengancam', 'rp 750', 'mengimbau', '11 45', 'kereta api', 'angkutan', 'perlintasan sebidang', 'kecelakaan ditetapkan', 'dikemudikan', 'palang pintu', '1835 ga', 'perlintasan', 'berbunyi', 'api', 'tertutup kereta', 'perlintasan kereta', 'sebidang', 'diharapkan kepedulian', 'pengguna jalan', 'musyawarah', 'api perlintasan', 'mengancam keselamatan', 'jalan', 'ditetapkan', 'keselamatan perlintasan', '396 nolog', 'angkutan jalan', 'kemarin', 'rel kereta', 'ditetapkan tersangka', 'sebidang sesuai', 'menciptakan', 'kecelakaan perlintasan', '17', 'joni kemacetan', 'lintas angkutan', 'dihubungi', '2009', '114 peraturan', 'mobil', 'confero', 'api jalan', 'bernopol', 'truk colt', 'palang', 'n', '2501', 'polres kediri', 'ga', '750', 'lintas perlintasan', 'ditumpangi', 'keluarga abg', 'mobil dikendarai', 'terseret', 'mengakibatkan', 'blitar kediri', 'tabrakan kereta', 'kediri akp', 'meter', 'mengaku', 'kediri penyelidikan', 'terbalik', 'kedepankan musyawarah', 'api peristiwa', 'sisi', 'juta', 'lintas', 'ongki', 'mobil mercedes', 'pintu', 'agus', 'kerusakan', 'melintas perlintasan', 'lokomotif kereta', 'dikendarai ongki', 'warga', 'dievakuasi', 'rusak parah', 'sedati', 'truk dievakuasi', 'jalan raya', 'sidoarjo', 'tertabrak', 'api sebidang', 'sang', 'bersamaan munculah', 'api rapih', 'melibatkan', 'api palang', 'menyebabkan kecelakaan', 'tol', 'bachtiar', 'berbunyi palang', 'pasal 114', 'kediri kecelakaan', 'dhoho', 'mengajak', 'sebidang kereta', 'blitar', 'mengimbau pengguna', 'mengalami luka', 'dihindarkan', 'bunyi', 'perjalanan kereta', 'akp', 'pintu dibuka', 'bobby', 'mercedes', 'ongki agus', 'zulfikar', 'kereta rusak', 'warga sedati', 'oukul', 'meninggalkan', 'arah barat', '15', 'menciptakan keselamatan', 'pengguna', '00', 'tersangka', 'kendaraan', 'berat', '30 wib']]\n",
      "Total Kandidat temp:  170\n",
      "kurang dari 80\n",
      "Total Kandidat final:  80\n",
      "Kandidat final:  ['50', '45', 'kereta', 'cc2039814', '1835', '41', 'munculah', 'terseret 50', 'kecelakaan', '50 meter', '45 wib', 'benz', 'pintu kereta', 'cc2039814 dimasinisi', '396', '41 warga', 'rapih', 'log cc2039814', 'benz menyebabkan', 'keluarga', '8438 dikemudikan', 'menerobosnya', 'ka 396', 'mengedepankan', 'bernopol 1835', '2501 melintas', 'mobil keluarga', 'kemacetan', 'agus 41', 'kepedulian', 'mengedepankan kedepankan', 'langsung menerobosnya', 'didahulukan', 'munculah kereta', 'terluka', 'mendahulukan kereta', 'pemerintah', 'mengungkap', 'kedepankan', 'kemacetan pengguna', 'mendahulukan', 'tetapkan', 'langsung', '000 keselamatan', 'mengemudikan', 'pengendara toyota', 'mengancam', 'rp 750', 'mengimbau', '11 45', 'kereta api', 'angkutan', 'perlintasan sebidang', 'kecelakaan ditetapkan', 'dikemudikan', 'palang pintu', '1835 ga', 'perlintasan', 'berbunyi', 'api', 'tertutup kereta', 'perlintasan kereta', 'sebidang', 'diharapkan kepedulian', 'pengguna jalan', 'musyawarah', 'api perlintasan', 'mengancam keselamatan', 'jalan', 'ditetapkan', 'keselamatan perlintasan', '396 nolog', 'angkutan jalan', 'kemarin', 'rel kereta', 'ditetapkan tersangka', 'sebidang sesuai', 'menciptakan', 'kecelakaan perlintasan', '17']\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# hasilDokumenWhen=[]\n",
    "hasilDokumenWho=[]\n",
    "# hasilDokumenWhere=[]\n",
    "# hasilDokumenWhat=[]\n",
    "\n",
    "# kueriAsliWhat='apa sebenarnya kejadian kecelakaan yang terjadi diberita tersebut'\n",
    "# kueriAsliWhat='musibah apa yang terjadi'\n",
    "# kueriAsliWhere='daerah lokasi tempat terjadinya kecelakaan'\n",
    "# kueriAsliWhere='kota kejadian terjadi'\n",
    "kueriAsliWho='siapa pelaku kejadian ini'\n",
    "# kueriAsliWhen='hari apa waktu terjadinya'\n",
    "\n",
    "# kandidatFixWhat,keywordGabungWhat,keywordBOW_What, hasilrankWhat, kueriFixWithDelimiter_What = kandidatFix(kueriAsliWhat, bow_kecelakaan_text)\n",
    "# kandidatFixWhere,keywordGabungWhere,keywordBOW_Where, hasilrankWhere,kueriFixWithDelimiter_Where = kandidatFix(kueriAsliWhere, bow_where_text)\n",
    "kandidatFixWho,keywordGabungWho,keywordBOW_Who, hasilrankWho, kueriFixWithDelimiter_Who = kandidatFix(kueriAsliWho, bow_who_text)\n",
    "# kandidatFixWhen,keywordGabungWhen,keywordBOW_When, hasilrankWhen, kueriFixWithDelimiter_When = kandidatFix(kueriAsliWhen, bow_when_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kejadian kecelakaan diberita']\n",
      "  (0, 6678)\t0.7510210317790929\n",
      "  (0, 6606)\t0.6602782821094958\n",
      "Yake  :  ['hati hati', 'mobil minibus', 'melenceng kiri', 'pas belok', 'belok tekor', 'warga berusaha', 'meninggal dunia', 'kereta api', 'kiri menabrak', 'kiri gungun', 'mobil', 'belok melenceng', 'kiri jalan', 'minibus melenceng', 'tekor belok', 'mengangkat mobil', 'mobil mobilnya', 'kendaraannya mobil', 'jalan sangkuriang', 'tekor melenceng', 'pengemudi hati', 'dunia erin', 'hati mengendalikan', 'hati menguasai', 'sangkuriang menyebrang', 'mobilnya pas', 'pas diangkat', 'dibawa warga', 'warga keluarganya', 'mobil melaju', 'citeureup mobil', 'badan mobil', 'menyebut mobil', 'oleng kiri', 'terhimpit coba', 'berusaha mengangkat', 'dunia perawatan', 'perawatan meninggal', 'korban', 'cimahi jumat', 'kota cimahi', 'mobilnya diangkat', 'hati', 'minibus persimpangan', 'susah warga', 'dingin pas', 'tabrakan warga', 'konsentrasi pas', 'polres cimahi', 'cimahi iptu']\n",
      "TFIDF :  ['cctv', 'video', 'api', 'wanita', 'paruh', 'baya', 'berjalan', 'persimpangan', 'jumat', '8', 'sore', 'peristiwa', 'terekam', 'kamera', 'milik', 'dinas', 'perhubungan', 'dishub', 'd', '1807', 'sgb', 'menyeruduk', '50', 'salah', 'mata', 'pemilik', 'warung', 'kupat', 'korbannya', 'pulang', 'beli', 'soto', 'citeureup', 'ya', 'wartawan']\n",
      "Bert  :  ['hati hati', 'mobil minibus', 'melenceng kiri', 'pas belok', 'belok tekor', 'warga berusaha', 'meninggal dunia', 'kereta api', 'kiri menabrak', 'kiri gungun', 'mobil', 'belok melenceng', 'kiri jalan', 'minibus melenceng', 'tekor belok', 'mengangkat mobil', 'mobil mobilnya', 'kendaraannya mobil', 'jalan sangkuriang', 'tekor melenceng', 'pengemudi hati', 'dunia erin', 'hati mengendalikan', 'hati menguasai', 'sangkuriang menyebrang', 'mobilnya pas', 'pas diangkat', 'dibawa warga', 'warga keluarganya', 'mobil melaju', 'citeureup mobil', 'badan mobil', 'menyebut mobil', 'oleng kiri', 'terhimpit coba', 'berusaha mengangkat', 'dunia perawatan', 'perawatan meninggal', 'korban', 'cimahi jumat', 'kota cimahi', 'mobilnya diangkat', 'hati', 'minibus persimpangan', 'susah warga', 'dingin pas', 'tabrakan warga', 'konsentrasi pas', 'polres cimahi', 'cimahi iptu']\n",
      "kandidat temp [['minibus', 'mobil minibus', 'minibus melenceng', 'minibus persimpangan', 'kecamatan', 'korban minibus', 'hyundai', 'kaliboto', 'minimarket', 'kaliboto kecamatan', 'mengindikasikan', 'minimarket mengangkat', 'perhubungan', 'kabupaten', 'kabupaten lumajang', 'bersangkutan', 'kecamatan jatiroto', 'mengendalikan', 'mengerikan', 'minibus nomor', 'mengangkat', 'kendaraan hyundai', 'tersangkut', 'mobil melaju', 'hyundai hitam', 'pintu', 'perhubungan dishub', 'menganalisis', 'berusaha mengangkat', 'parkir minimarket', 'warga keluarganya', 'keluarganya', 'jatiroto kabupaten', 'cctv kecelakaan', 'lumajang', 'pengendara motor', 'melepaskan', 'tertutup mengindikasikan', 'dashcam', 'menganalisis kerusakan', 'sekuriti', 'motor tersangkut', 'terluka', 'menenangkan', '2021', 'jumat 2021', 'kecelakaan', 'kronologinya mengendarai', 'tenangkan', 'hati hati', 'kerusakan', 'menyelamatkan video', 'melenceng kiri', 'gungun', 'pas belok', 'melepaskan tembakan', 'belok tekor', 'wartawan', 'warga berusaha', 'anggota keluarganya', 'meninggal dunia', 'pengemudi', 'kereta api', 'gungun kbo', 'kiri menabrak', 'melaju', 'kiri gungun', 'rekaman cctv', 'mobil', 'pengendara', 'belok melenceng', 'mengendarai motor', 'kiri jalan', 'penembakan', 'kbo satlantas', 'tekor belok', 'penyebabnya', 'mengangkat mobil', 'cctv', 'tertabrak pengemudi', 'mobil mobilnya', 'video', 'menguasai', 'kendaraannya mobil', 'api', 'rumah bersangkutan', 'jalan sangkuriang', 'wanita', 'langsung', 'tekor melenceng', 'paruh', 'dinas perhubungan', 'pengemudi hati', 'baya', 'tembakan', 'dunia erin', 'berjalan', 'bodi motor', 'hati mengendalikan', 'persimpangan', 'jatiroto', 'hati menguasai', 'jumat', 'desember 2021', 'sangkuriang menyebrang', '8', 'pelaku', 'mobilnya pas', 'sore', 'motor melewati', 'pas diangkat', 'peristiwa', 'kbo', 'dibawa warga', 'terekam', 'pelaku melepaskan', 'kamera', 'kereta', 'milik', 'motornya tertahan', 'citeureup mobil', 'dinas', 'motornya', 'badan mobil', 'sangkuriang kecepatan', 'menyebut mobil', 'dishub', 'sangkuriang', 'oleng kiri', 'd', 'indonesia video', 'terhimpit coba', '1807', 'tangerang', 'sgb', 'perlintasan motornya', 'dunia perawatan', 'menyeruduk', 'kronologis', 'perawatan meninggal', '50', 'penembakan pelaku', 'korban', 'salah', 'mengendarai', 'cimahi jumat', 'mata', 'menyelamatkan motor', 'kota cimahi', 'pemilik', 'menyeberangi', 'mobilnya diangkat', 'warung', 'menyeberangi perlintasan', 'hati', 'kupat', 'menyeberang', 'korbannya', 'beruntungnya menyelamatkan', 'susah warga', 'pulang', 'membuang', 'dingin pas', 'beli', 'kamera cctv', 'tabrakan warga', 'soto', 'purnomo', 'konsentrasi pas', 'citeureup', 'mobil tersangkut', 'polres cimahi', 'ya', 'nugroho', 'cimahi iptu', 'ceroboh tertabrak']]\n",
      "Total Kandidat temp:  176\n",
      "kurang dari 80\n",
      "Total Kandidat final:  80\n",
      "Kandidat final:  ['minibus', 'mobil minibus', 'minibus melenceng', 'minibus persimpangan', 'kecamatan', 'korban minibus', 'hyundai', 'kaliboto', 'minimarket', 'kaliboto kecamatan', 'mengindikasikan', 'minimarket mengangkat', 'perhubungan', 'kabupaten', 'kabupaten lumajang', 'bersangkutan', 'kecamatan jatiroto', 'mengendalikan', 'mengerikan', 'minibus nomor', 'mengangkat', 'kendaraan hyundai', 'tersangkut', 'mobil melaju', 'hyundai hitam', 'pintu', 'perhubungan dishub', 'menganalisis', 'berusaha mengangkat', 'parkir minimarket', 'warga keluarganya', 'keluarganya', 'jatiroto kabupaten', 'cctv kecelakaan', 'lumajang', 'pengendara motor', 'melepaskan', 'tertutup mengindikasikan', 'dashcam', 'menganalisis kerusakan', 'sekuriti', 'motor tersangkut', 'terluka', 'menenangkan', '2021', 'jumat 2021', 'kecelakaan', 'kronologinya mengendarai', 'tenangkan', 'hati hati', 'kerusakan', 'menyelamatkan video', 'melenceng kiri', 'gungun', 'pas belok', 'melepaskan tembakan', 'belok tekor', 'wartawan', 'warga berusaha', 'anggota keluarganya', 'meninggal dunia', 'pengemudi', 'kereta api', 'gungun kbo', 'kiri menabrak', 'melaju', 'kiri gungun', 'rekaman cctv', 'mobil', 'pengendara', 'belok melenceng', 'mengendarai motor', 'kiri jalan', 'penembakan', 'kbo satlantas', 'tekor belok', 'penyebabnya', 'mengangkat mobil', 'cctv', 'tertabrak pengemudi']\n",
      "Rank  :  ['minibus', 'mobil minibus', 'minibus melenceng', 'minibus persimpangan', 'kecamatan', 'korban minibus', 'hyundai', 'kaliboto', 'minimarket', 'kaliboto kecamatan', 'mengindikasikan', 'minimarket mengangkat', 'perhubungan', 'kabupaten', 'kabupaten lumajang', 'bersangkutan', 'kecamatan jatiroto', 'mengendalikan', 'mengerikan', 'minibus nomor', 'mengangkat', 'kendaraan hyundai', 'tersangkut', 'mobil melaju', 'hyundai hitam', 'pintu', 'perhubungan dishub', 'menganalisis', 'berusaha mengangkat', 'parkir minimarket', 'warga keluarganya', 'keluarganya', 'jatiroto kabupaten', 'cctv kecelakaan', 'lumajang', 'pengendara motor', 'melepaskan', 'tertutup mengindikasikan', 'dashcam', 'menganalisis kerusakan', 'sekuriti', 'motor tersangkut', 'terluka', 'menenangkan', '2021', 'jumat 2021', 'kecelakaan', 'kronologinya mengendarai', 'tenangkan', 'hati hati', 'kerusakan', 'menyelamatkan video', 'melenceng kiri', 'gungun', 'pas belok', 'melepaskan tembakan', 'belok tekor', 'wartawan', 'warga berusaha', 'anggota keluarganya', 'meninggal dunia', 'pengemudi', 'kereta api', 'gungun kbo', 'kiri menabrak', 'melaju', 'kiri gungun', 'rekaman cctv', 'mobil', 'pengendara', 'belok melenceng', 'mengendarai motor', 'kiri jalan', 'penembakan', 'kbo satlantas', 'tekor belok', 'penyebabnya', 'mengangkat mobil', 'cctv', 'tertabrak pengemudi']\n"
     ]
    }
   ],
   "source": [
    "hasilDokumenWho=[]\n",
    "kueriAsli='apa sebenarnya kejadian kecelakaan yang terjadi diberita tersebut'\n",
    "\n",
    "kueri=preprocessing(kueriAsli)\n",
    "kueri= [\" \".join (kueri)]\n",
    "print (kueri)\n",
    "hasilkandidat=[]\n",
    "keywordGabung=[]\n",
    "kandidatFix=[]\n",
    "kueriFix=[]\n",
    "kueriFixWithDelimiter=[]\n",
    "\n",
    "\n",
    "\n",
    "hasilSearch=cari_dokpertama(kueriAsli)\n",
    "keywordYake=keyword_yake(hasilSearch)\n",
    "keywordtfidf2=keyword_tfidf(hasilSearch)\n",
    "keywordbert=keyword_bert (hasilSearch)\n",
    "\n",
    "print(\"Yake  : \", keywordYake)\n",
    "print(\"TFIDF : \", keywordtfidf2)\n",
    "print(\"Bert  : \", keywordYake)\n",
    "# keywordBOW=keyword_BOW(bow, kueri)\n",
    "\n",
    "keywordGabung.append(keywordYake)\n",
    "keywordGabung.append(keywordtfidf2)\n",
    "keywordGabung.append(keywordbert)\n",
    "\n",
    "hasilrank=rangking(keywordGabung)\n",
    "print(\"Rank  : \", hasilrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=1\n",
    "\n",
    "testing_data = []\n",
    "\n",
    "tfidf_vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "#What\n",
    "# for i in range(0, len(document_text_test)-1):\n",
    "    \n",
    "#     hasilWhat=[]\n",
    "\n",
    "#     teks=df_test.iloc[i,-2]\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "#     query_vec_What= tfidf_vectorizer.transform(kandidatFixWhat)\n",
    "#     results_what=cosine_similarity(tfidf_matrix, query_vec_What).reshape((-1))\n",
    "#     hasilDokumenWhat.append(df_test.iloc[i,2])\n",
    "#     for a in kueriFixWithDelimiter_What:\n",
    "#         cariW = re.findall(a,hasilDokumenWhat[i])\n",
    "#         #print(cariW)\n",
    "#         if cariW:\n",
    "#             hasilWhat.append(a)\n",
    "    \n",
    "#     data = [i,df_test.iloc[i,2],'what',kueriAsliWhat,keywordBOW_What , keywordGabungWhat,kandidatFixWhat, hasilWhat, results_what,' ',' ']\n",
    "\n",
    "#     testing_data.append(data)\n",
    "\n",
    "#Where\n",
    "# for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "#     hasilWhere=[]\n",
    "#     truePos = 0\n",
    "\n",
    "#     teks=df_test.iloc[i,-2]\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "#     query_vec_Where= tfidf_vectorizer.transform(kandidatFixWhere)\n",
    "#     results_where=cosine_similarity(tfidf_matrix, query_vec_Where).reshape((-1))\n",
    "#     hasilDokumenWhere.append(df_test.iloc[i,2])\n",
    "#     for a in kueriFixWithDelimiter_Where:\n",
    "#         cariW = re.findall(a,hasilDokumenWhere[i])\n",
    "#         #print(cariW)\n",
    "#         if cariW:\n",
    "#             hasilWhere.append(a)\n",
    "#     # print(hasilWhere)\n",
    "#     for x in hasilWhere:\n",
    "#         for y in keywordBOW_Where:\n",
    "#             if(x == y):\n",
    "#                 truePos=1\n",
    "    \n",
    "#     data = [i,df_test.iloc[i,2],'where',kueriAsliWhere,keywordBOW_Where , keywordGabungWhere, kandidatFixWhere, hasilWhere, results_where,truePos,' ']\n",
    "\n",
    "#     testing_data.append(data)\n",
    "\n",
    "#Who\n",
    "for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "    hasilWho=[]\n",
    "\n",
    "    teks=df_test.iloc[i,-2]\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "    query_vec_Who= tfidf_vectorizer.transform(kandidatFixWho)\n",
    "    results_who=cosine_similarity(tfidf_matrix, query_vec_Who).reshape((-1))\n",
    "    hasilDokumenWho.append(df_test.iloc[i,2])\n",
    "    for a in kueriFixWithDelimiter_Who:\n",
    "        cariW = re.findall(a,hasilDokumenWho[i])\n",
    "        #print(cariW)\n",
    "        if cariW:\n",
    "            hasilWho.append(a)\n",
    "    \n",
    "    data = [i,df_test.iloc[i,2],'who',kueriAsliWho,keywordBOW_Who , keywordGabungWho, kandidatFixWho, hasilWho, results_who,' ',' ']\n",
    "\n",
    "    testing_data.append(data)\n",
    "\n",
    "#When\n",
    "# for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "#     hasilWhen=[]\n",
    "\n",
    "#     teks=df_test.iloc[i,-2]\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "#     query_vec_When= tfidf_vectorizer.transform(kandidatFixWhen)\n",
    "#     results_when=cosine_similarity(tfidf_matrix, query_vec_When).reshape((-1))\n",
    "#     hasilDokumenWhen.append(df_test.iloc[i,2])\n",
    "#     for a in kueriFixWithDelimiter_When:\n",
    "#         cariW = re.findall(a,hasilDokumenWhen[i])\n",
    "#         #print(cariW)\n",
    "#         if cariW:\n",
    "#             hasilWhen.append(a)\n",
    "    \n",
    "#     data = [i,df_test.iloc[i,2],'when',kueriAsliWhen,keywordBOW_When , keywordGabungWhen, kandidatFixWhen, hasilWhen, results_when,' ',' ']\n",
    "\n",
    "#     testing_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv\n",
    "writer = pd.DataFrame(testing_data, columns=['No Document','Description', 'W','Pertanyaan', 'Keyword BOW', 'Keyword Gabung','kandidat fix','hasilW', 'Kemiripan', 'True Positif', 'True Negative'])\n",
    "writer.to_csv('QE_Stat_testing_who_result.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
