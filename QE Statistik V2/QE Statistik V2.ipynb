{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library-library\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Data Preparation and Preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "# Word Embedding\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and Expansion Query\n",
    "import nltk\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from yake import KeywordExtractor\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = [\"tempat\", \"waktu\", \"gempa\", \"banjir\"]\n",
    "\n",
    "NLTK_StopWords = stopwords.words('indonesian')\n",
    "NLTK_StopWords.extend([\"detik\", \"detikjatim\", \"detikjateng\", \"detikjabar\", \"detiksulsel\", \"detiksumbar\", \"detikbali\", \"detikpapua\", \"detiksulteng\", \"detikmaluku\", \"detjatim\", \"detikcom\", \"allahumma\", \"aamiin\", \"allah\", \"bismillah\"])\n",
    "NLTK_StopWords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "NLTK_StopWords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "NLTK_StopWords = set(NLTK_StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita):\n",
    "    # Preprocessing\n",
    "    s = berita.lower()\n",
    "    s = s.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    s = s.encode('ascii', 'replace').decode('ascii')\n",
    "    ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", s).split())\n",
    "    s.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    s = s.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\b\\w{1,1}\\b', '', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "    # Stopwords checking\n",
    "    T = [t for t in tokens if ((t in excluded_words) or (t not in NLTK_StopWords))]\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1783 entries, 0 to 1782\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1783 non-null   object\n",
      " 1   date         1783 non-null   object\n",
      " 2   description  1783 non-null   object\n",
      " 3   source       1783 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.6+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1604\n"
     ]
    }
   ],
   "source": [
    "df_total = pd.read_csv('corpus/dataset/df_total.csv')\n",
    "df_total = df_total[pd.notnull(df_total['description'])]\n",
    "print(df_total.info())\n",
    "print ('-'*90)\n",
    "document_text= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 0 to 178\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        179 non-null    object\n",
      " 1   date         179 non-null    object\n",
      " 2   description  179 non-null    object\n",
      " 3   source       179 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.0+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('corpus/dataset/df_test.csv')\n",
    "df_test = df_test[pd.notnull(df_test['description'])]\n",
    "print(df_test.info())\n",
    "print ('-'*90)\n",
    "document_text_test= joblib.load('corpus/model/desc_text_test.pkl')\n",
    "print(len(document_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1604 entries, 0 to 1603\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1604 non-null   object\n",
      " 1   date         1604 non-null   object\n",
      " 2   description  1604 non-null   object\n",
      " 3   source       1604 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 62.7+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1604\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('corpus/dataset/df_train.csv')\n",
    "df_train = df_train[pd.notnull(df_train['description'])]\n",
    "print(df_train.info())\n",
    "print ('-'*90)\n",
    "document_text_train= joblib.load('corpus/model/desc_text_train.pkl')\n",
    "print(len(document_text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17485 entries, 0 to 17484\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Unnamed: 0              17485 non-null  int64 \n",
      " 1   tingkat setelah parent  17485 non-null  int64 \n",
      " 2   parent                  17485 non-null  object\n",
      " 3   similarity              17485 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 546.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_what = pd.read_csv(\"bow/bow_what.csv\")\n",
    "df_bow_what.head()\n",
    "df_bow_what.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26186 entries, 0 to 26185\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Unnamed: 0              26186 non-null  int64 \n",
      " 1   tingkat setelah parent  26186 non-null  int64 \n",
      " 2   parent                  26185 non-null  object\n",
      " 3   similarity              26186 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 818.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_when = pd.read_csv(\"bow/bow_when.csv\")\n",
    "df_bow_when.head()\n",
    "df_bow_when.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9786 entries, 0 to 9785\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Unnamed: 0              9786 non-null   int64 \n",
      " 1   tingkat setelah parent  9786 non-null   int64 \n",
      " 2   parent                  9786 non-null   object\n",
      " 3   similarity              9786 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 305.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_where = pd.read_csv(\"bow/bow_where.csv\")\n",
    "df_bow_where.head()\n",
    "df_bow_where.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17293 entries, 0 to 17292\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Unnamed: 0              17293 non-null  int64 \n",
      " 1   tingkat setelah parent  17293 non-null  int64 \n",
      " 2   parent                  17293 non-null  object\n",
      " 3   similarity              17293 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 540.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load bow dataset\n",
    "df_bow_who = pd.read_csv(\"bow/bow_who.csv\")\n",
    "df_bow_who.head()\n",
    "df_bow_who.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What  bencana\n",
      "When  waktu\n",
      "Where  surabaya\n",
      "Who  korban\n",
      "{'What': 17485, 'When': 26186, 'Where': 9786, 'Who': 17293}\n"
     ]
    }
   ],
   "source": [
    "# Ambil parent dari bow\n",
    "bow_list_what = []\n",
    "bow_list_when = []\n",
    "bow_list_where = []\n",
    "bow_list_who = []\n",
    "\n",
    "for i in range(0, df_bow_what.shape[0]):\n",
    "  bow_list_what.append(df_bow_what.iloc[i, 2])\n",
    "\n",
    "for i in range(0, df_bow_when.shape[0]):\n",
    "  bow_list_when.append(df_bow_when.iloc[i, 2])\n",
    "\n",
    "for i in range(0, df_bow_where.shape[0]):\n",
    "  bow_list_where.append(df_bow_where.iloc[i, 2])\n",
    "\n",
    "for i in range(0, df_bow_who.shape[0]):\n",
    "  bow_list_who.append(df_bow_who.iloc[i, 2])\n",
    "\n",
    "print(\"What \", bow_list_what[0])\n",
    "print(\"When \", bow_list_when[0])\n",
    "print(\"Where \", bow_list_where[0])\n",
    "print(\"Who \", bow_list_who[0])\n",
    "\n",
    "print({\n",
    "  'What': len(bow_list_what),\n",
    "  'When': len(bow_list_when),\n",
    "  'Where': len(bow_list_where),\n",
    "  'Who': len(bow_list_who)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use data train\n",
    "def cari_dokpertama(kueriAsli: str) -> list[str]:\n",
    "    kueriPre = preprocessing(kueriAsli)\n",
    "    kueriPre = \" \".join(kueriPre)\n",
    "    hasilSearch = []\n",
    "    tfidf_matrix = joblib.load('corpus/matrix/tfidf_train.pkl')\n",
    "    tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "    query_vec = tfidf_vectorizer.transform([kueriPre])\n",
    "    results = cosine_similarity(tfidf_matrix, query_vec).reshape((-1))\n",
    "    for i in results.argsort()[-5:][::-1]:\n",
    "        hasilSearch.append(df_total.iloc[i,-2])\n",
    "    hasilSearch=\". \".join(hasilSearch)\n",
    "    \n",
    "    return hasilSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Keywords Extraction with YAKE\n",
    "def keyword_yake(hasilSearch: str) -> list[str]:\n",
    "    keywordYake=[]\n",
    "\n",
    "    k_extractor = KeywordExtractor(lan=\"id\", n=1, top=10)\n",
    "    k_extractor2 = KeywordExtractor(lan=\"id\", n=2, top=10)\n",
    "    keywords = k_extractor.extract_keywords(text=hasilSearch)\n",
    "    keywords = k_extractor2.extract_keywords(text=hasilSearch)\n",
    "    keywordYake = [x for x, y in keywords]\n",
    "    #keywordYake.append(keywords)\n",
    "    #print (keywordYake)\n",
    "    return keywordYake\n",
    "#print(\"Keywords of article\\n\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with TFIDF\n",
    "def keyword_tfidf(hasilSearch: str) -> list[str]:\n",
    "\n",
    "    keywordtfidf=[]\n",
    "    keywordtfidf2=[]\n",
    "\n",
    "    total_words = re.sub(r'[^\\w]', ' ', hasilSearch)\n",
    "    total_words = total_words.lower().split()\n",
    "    #print (total_words)\n",
    "    total_word_length = len(total_words)\n",
    "    total_sentences = tokenize.sent_tokenize(hasilSearch)\n",
    "    total_sent_len = len(total_sentences)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    def check_sent(word, sentences): \n",
    "        final = [all([w in x for w in word]) for x in sentences] \n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "        return int(len(sent_len))\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if (each_word in excluded_words) or (each_word not in NLTK_StopWords):\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "    #print(idf_score)\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    #print(tf_idf_score)\n",
    "    def get_top_n(dict_elem, n):\n",
    "        result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "        hasil =list(result.keys())\n",
    "        #print(list(result.keys()))        \n",
    "        return hasil\n",
    "    #print(get_top_n(tf_idf_score, 25))\n",
    "    #print(len(get_top_n(tf_idf_score, 1)))\n",
    "    keywordtfidf.append(get_top_n(tf_idf_score, 25))\n",
    "    for i in range(len(keywordtfidf)):\n",
    "        #print (i)\n",
    "        totalKw=0\n",
    "        totalKw=len(keywordtfidf[i])\n",
    "        for j in range(totalKw):\n",
    "            #print (j)\n",
    "            keywordtfidf2.append(keywordtfidf[i][j])\n",
    "    #print (keywordtfidf2)\n",
    "    return keywordtfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with BERT\n",
    "def keyword_bert(hasilSearch: str) -> list[str]:\n",
    "\n",
    "    keywordbert=[]\n",
    "\n",
    "    #for j in range(len(array_text)):\n",
    "    keyword1 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 1))\n",
    "    keyword2 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "    #print(\"Keywords of article\\n\", keywords)\n",
    "    for i in range (0,len (keyword1)):\n",
    "        keywordbert.append(keyword1[i][0])\n",
    "        keywordbert.append(keyword2[i][0])\n",
    "    #print (keywordbert)\n",
    "    return keywordbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rangking (keywordGabung: list[str], kueriAsli: str) -> list[str]:\n",
    "    kandidatFinalCek=[]\n",
    "    kandidatFinalFix=[]\n",
    "    for i in keywordGabung:\n",
    "        if (i not in kandidatFinalCek and i!=0):\n",
    "            kandidatFinalCek.append(i)\n",
    "    queries=[kueriAsli]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(kandidatFinalCek)\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 30\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFinalFix.append(kandidatFinalCek[idx])\n",
    "\n",
    "    print ('Kandidat Final Fix Rank: ', kandidatFinalFix)\n",
    "    return kandidatFinalFix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordCustomBow(bowList: list[str], initialQuery: str) -> list[str]:\n",
    "    cekDuplicate = []\n",
    "    kandidatFix = []\n",
    "\n",
    "    for i in bowList:\n",
    "        if(i not in cekDuplicate and i!=0):\n",
    "            cekDuplicate.append(i)\n",
    "\n",
    "    queries=[initialQuery]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(cekDuplicate)\n",
    "    \n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 10\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFix.append(cekDuplicate[idx])\n",
    "    \n",
    "    print ('Kandidat Final Fix BoW: ', kandidatFix)\n",
    "    return kandidatFix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bencana berita\n",
      "waktu berita\n",
      "daerah tempat berita\n",
      "personel lembaga terlibat berita\n"
     ]
    }
   ],
   "source": [
    "# Creating query for what, when, where, who\n",
    "what_initial_query = \"bencana apa yang terjadi dalam berita\"\n",
    "when_initial_query = \"kapan waktu berita tersebut terjadi\"\n",
    "where_initial_query = \"di daerah mana tempat berita itu terjadi\"\n",
    "who_initial_query = \"siapa orang, personel, atau lembaga yang terlibat dalam berita\"\n",
    "\n",
    "what_query = preprocessing(what_initial_query)\n",
    "what_query = \" \".join(what_query)\n",
    "print (what_query)\n",
    "\n",
    "when_query = preprocessing(when_initial_query)\n",
    "when_query = \" \".join(when_query)\n",
    "print (when_query)\n",
    "\n",
    "where_query = preprocessing(where_initial_query)\n",
    "where_query = \" \".join(where_query)\n",
    "print (where_query)\n",
    "\n",
    "who_query = preprocessing(who_initial_query)\n",
    "who_query = \" \".join(who_query)\n",
    "print (who_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepareWData(initial_query: str, bow_list: list[str]):\n",
    "    hasilkandidat = []\n",
    "    keywordGabung = []\n",
    "    qeGabungan = []\n",
    "    kueriFix = []\n",
    "\n",
    "    hasilSearch     = cari_dokpertama(initial_query)\n",
    "    # (ini yake + tfidf + bert) = qe statistik\n",
    "    keywordYake     = keyword_yake(hasilSearch) # 20\n",
    "    keywordtfidf2   = keyword_tfidf(hasilSearch) # 20\n",
    "    keywordbert     = keyword_bert(hasilSearch) # 20\n",
    "    # ini qe bow\n",
    "    keywordBoW      = keywordCustomBow(bow_list, initial_query)\n",
    "\n",
    "    for keyword in keywordYake:\n",
    "        keywordGabung.append(keyword)\n",
    "    for keyword in keywordtfidf2:\n",
    "        keywordGabung.append(keyword)\n",
    "    for keyword in keywordbert:\n",
    "        keywordGabung.append(keyword)  \n",
    "\n",
    "    # hasilrank = qe statistik\n",
    "    hasilrank = rangking(keywordGabung, initial_query)\n",
    "    \n",
    "    for word in hasilrank:\n",
    "        kueriFix.append(word)\n",
    "\n",
    "    for word in kueriFix:\n",
    "        hasilkandidat.append(word)\n",
    "\n",
    "    kueriFix = [preprocessing(i) for i in kueriFix]\n",
    "    \n",
    "    for word in kueriFix:\n",
    "        for subWord in word:\n",
    "            qeGabungan.append(subWord)\n",
    "\n",
    "    for word in keywordBoW:\n",
    "        qeGabungan.append(word)\n",
    "\n",
    "    # (hasil ranking + bow) = kandidat final\n",
    "    qeGabungan = [\" \".join(qeGabungan)]\n",
    "\n",
    "    print('*'*120)\n",
    "    qeStatistik = hasilrank\n",
    "    qeBoW = keywordBoW\n",
    "\n",
    "    return [qeGabungan, qeStatistik, qeBoW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kandidat Final Fix BoW:  ['bencana', 'malapetaka', 'suryakencana', 'keniscayaan', 'tragis', 'musibah', 'kebinasaan', 'kelangkaan', 'informations', 'disaster']\n",
      "Kandidat Final Fix Rank:  ['tanggap darurat', 'gempa', 'validasi', 'info', 'lumajang', 'bermagnitudo', 'serambagiantimurmaluku', 'daya', 'status tanggap', 'pemprov', 'sr', 'diguncang', 'cuit', 'lebak', 'lumajang diguncang', 'bmkgjogja', 'daya lumajang', 'verifikasi', 'terverifikasi', 'berlokasi', 'wib cuit', 'mengguncang', 'lintang selatan', 'mag36', 'cuit bmkgjogja', 'alit', 'bujur timur', '11295 bujur', '183656', '919']\n",
      "************************************************************************************************************************\n",
      "Kandidat Final Fix BoW:  ['wiraswastawan', 'wiracaritawan', 'pelaporan', 'wartawan', 'investigatif', 'personelnya', 'hartawan', 'jurnalis', 'stafnya', 'investigative']\n",
      "Kandidat Final Fix Rank:  ['laporan', 'tulis stasiun', 'twitter', 'tulis', 'info', 'stasiun', 'notification', 'menimbulkan', 'keterangannya', 'karangkates menyebut', 'bencana', 'masyarakat menerima', 'kabupaten', 'menyebut', 'bappeda kebakaran', 'bt', 'perencanaan', 'dirasakan tulis', 'kota madiun', 'madiun kedalaman', 'ls', 'masyarakat mengapresiasi', 'akun', 'dikonfirmasi', 'klimatologi', 'pembangunan daerah', 'bmkg karangkates', 'titik', 'kedalaman', 'kebakaran']\n",
      "************************************************************************************************************************\n",
      "Kandidat Final Fix BoW:  ['jurnalistiknya', 'akhbar', 'pelaporan', 'zamannya', 'zaman', 'ajalnya', 'ajal', 'waktunya', 'masanya', 'ajalku']\n",
      "Kandidat Final Fix Rank:  ['kejadian', 'tegal jawa', 'kombes', 'tulungagung', 'perkara', 'mengecek', 'jawa', 'tegal', 'mobil', 'jakarta', 'wib', 'kabupaten', 'pasuruan', 'kecamatan', 'tempat', 'purnomo', 'nelayan', 'laka', 'dirlantas', 'melaut', 'polda', 'kabupaten kediri', 'jakarta pusat', 'metro jaya', 'kebutuhan', 'cek', 'kabupaten pasuruan', 'berkoordinasi', 'kecelakaan', 'polda metro']\n",
      "************************************************************************************************************************\n",
      "Kandidat Final Fix BoW:  ['stiria', 'noticias', 'satawan', 'sastranegara', 'mondulkiri', 'daerahnya', 'masayarakat', 'daerah', 'kraje≈Ñskie', 'kawasan']\n",
      "Kandidat Final Fix Rank:  ['kawasan alunalun', 'kantor', 'kejadian', 'kabupaten', 'desa', 'padam', 'kejadiannya', 'cipada', 'roboh', 'mengguyur', 'bandung barat', 'sore', 'alunalun lembang', 'laporan', 'warga', 'titik', 'lembang dano', 'puting beliung', 'aliran', 'bpbd kabupaten', 'kamis', 'menentukan', 'kilometer', 'tenggelam', 'prabowo', 'puting', 'kerusakan puting', 'berjarak', 'angin puting', 'kerusakannya']\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "whatResultList = prepareWData(what_query, bow_list_what)\n",
    "print(\"What\")\n",
    "print()\n",
    "qeGabunganWhat = whatResultList[0]\n",
    "qeStatistikWhat = whatResultList[1]\n",
    "qeBoWWhat = whatResultList[2]\n",
    "\n",
    "whoResultList = prepareWData(who_query, bow_list_who)\n",
    "print(\"Who\")\n",
    "print()\n",
    "qeGabunganWho = whoResultList[0]\n",
    "qeStatistikWho = whoResultList[1]\n",
    "qeBoWWho = whoResultList[2]\n",
    "\n",
    "whenResultList = prepareWData(when_query, bow_list_when)\n",
    "print(\"When\")\n",
    "print()\n",
    "qeGabunganWhen = whenResultList[0]\n",
    "qeStatistikWhen = whenResultList[1]\n",
    "qeBoWWhen = whenResultList[2]\n",
    "\n",
    "whereResultList = prepareWData(where_query, bow_list_where)\n",
    "print(\"Where\")\n",
    "print()\n",
    "qeGabunganWhere = whereResultList[0]\n",
    "qeStatistikWhere = whereResultList[1]\n",
    "qeBoWWhere = whereResultList[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wCalculation():\n",
    "    document_result = []\n",
    "\n",
    "    document_result_w = []\n",
    "\n",
    "    for i in range(0, len(document_text_test)-1):\n",
    "        hasilWhat = []\n",
    "        hasilWho = []\n",
    "        hasilWhen = []\n",
    "        hasilWhere = []\n",
    "\n",
    "        teks = df_total.iloc[i, -2]\n",
    "        tfidf_vectorizer = joblib.load('corpus/vectorizer/vectorizer.pkl')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "        document_result_w.append(df_total.iloc[i, -2])\n",
    "\n",
    "        query_vec_what = tfidf_vectorizer.transform(qeGabunganWhat)\n",
    "        results_what = cosine_similarity(tfidf_matrix, query_vec_what).reshape((-1))\n",
    "\n",
    "        for a in qeStatistikWhat:\n",
    "            cariW = re.findall(a, document_result_w[i])\n",
    "            if cariW:\n",
    "                hasilWhat.append(a)\n",
    "\n",
    "        document_result.append([i, 'what', what_query, qeGabunganWhat, qeStatistikWhat, qeBoWWhat, hasilWhat, results_what, 0, 0, \"\"])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_who = tfidf_vectorizer.transform(qeGabunganWho)\n",
    "        results_who = cosine_similarity(tfidf_matrix, query_vec_who).reshape((-1))\n",
    "\n",
    "        for a in qeStatistikWho:\n",
    "            cariW = re.findall(a, document_result_w[i])\n",
    "            if cariW:\n",
    "                hasilWho.append(a)\n",
    "\n",
    "        document_result.append([i, 'who', who_query, qeGabunganWho, qeStatistikWho, qeBoWWho, hasilWho, results_who, 0, 0, \"\"])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_when = tfidf_vectorizer.transform(qeGabunganWhen)\n",
    "        results_when = cosine_similarity(tfidf_matrix, query_vec_when).reshape((-1))\n",
    "\n",
    "        for a in qeStatistikWhen:\n",
    "            cariW = re.findall(a, document_result_w[i])\n",
    "            if cariW:\n",
    "                hasilWhen.append(a)\n",
    "\n",
    "        document_result.append([i, 'when', when_query, qeGabunganWhen, qeStatistikWhen, qeBoWWhen, hasilWho, results_who, 0, 0, \"\"])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        query_vec_where = tfidf_vectorizer.transform(qeGabunganWhere)\n",
    "        results_where = cosine_similarity(tfidf_matrix, query_vec_where).reshape((-1))\n",
    "\n",
    "        for a in qeStatistikWhere:\n",
    "            cariW = re.findall(a, document_result_w[i])\n",
    "            if cariW:\n",
    "                hasilWhere.append(a)\n",
    "\n",
    "        document_result.append([i, 'where', where_query, qeGabunganWhere, qeStatistikWhere, qeBoWWhere, hasilWhere, results_where, 0, 0, \"\"])\n",
    "    \n",
    "    writer = pd.DataFrame(document_result, columns=['Data','W', 'Query', 'QE Gabungan', 'QE Bow', 'QE Statistik', 'Hasil Query', 'Similarity', 'True Positive', 'True Negative', 'Skimming News'])\n",
    "    writer.to_csv('QE_Stat_V2_testing_result.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "wCalculation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
