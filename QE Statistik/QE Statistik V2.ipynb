{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import library-library\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation and Preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "# Word Embedding\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and Expansion Query\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from yake import KeywordExtractor\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "NLTK_StopWords = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(berita):\n",
    "    s = berita.lower()\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = s.replace('\\r', ' ')\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    T = [t for t in tokens if (((t.lower() == \"tempat\") or (t.lower() == \"waktu\")) or (t not in NLTK_StopWords))]\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 147 entries, 0 to 146\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        147 non-null    object\n",
      " 1   date         147 non-null    object\n",
      " 2   description  147 non-null    object\n",
      " 3   source       147 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 5.7+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_test = df_test[pd.notnull(df_test['description'])]\n",
    "print(df_test.info())\n",
    "print ('-'*90)\n",
    "document_text_test= joblib.load('document_text_test.pkl')\n",
    "print(len(document_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1314 entries, 0 to 1313\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        1314 non-null   object\n",
      " 1   date         1314 non-null   object\n",
      " 2   description  1314 non-null   object\n",
      " 3   source       1314 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 51.3+ KB\n",
      "None\n",
      "------------------------------------------------------------------------------------------\n",
      "1314\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train = df_train[pd.notnull(df_train['description'])]\n",
    "print(df_train.info())\n",
    "print ('-'*90)\n",
    "document_text_train= joblib.load('document_text_train.pkl')\n",
    "print(len(document_text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tingkat setelah parent</th>\n",
       "      <th>parent</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ekonomi</td>\n",
       "      <td>[('perekonomian', 0.7561678290367126), ('ekono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>perekonomian</td>\n",
       "      <td>[('ekonomi', 0.7561678290367126), ('ekonominya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ekonominya</td>\n",
       "      <td>[('perekonomian', 0.676762044429779), ('ekonom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>politik</td>\n",
       "      <td>[('politiknya', 0.6882383227348328), ('sosial'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>sosial</td>\n",
       "      <td>[('sosialnya', 0.6959102153778076), ('politik'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tingkat setelah parent        parent  \\\n",
       "0                       1       ekonomi   \n",
       "1                       2  perekonomian   \n",
       "2                       2    ekonominya   \n",
       "3                       2       politik   \n",
       "4                       2        sosial   \n",
       "\n",
       "                                          similarity  \n",
       "0  [('perekonomian', 0.7561678290367126), ('ekono...  \n",
       "1  [('ekonomi', 0.7561678290367126), ('ekonominya...  \n",
       "2  [('perekonomian', 0.676762044429779), ('ekonom...  \n",
       "3  [('politiknya', 0.6882383227348328), ('sosial'...  \n",
       "4  [('sosialnya', 0.6959102153778076), ('politik'...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_ekonomi_read = pd.read_csv('bow_ekonomi.csv')\n",
    "bow_ekonomi_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ekonomi_text = []\n",
    "\n",
    "for i in range(0, bow_ekonomi_read.shape[0]):\n",
    "    bow_ekonomi_text.append(bow_ekonomi_read.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_where_read = pd.read_csv('bow_where.csv')\n",
    "bow_where_text = []\n",
    "\n",
    "for i in range(0, bow_where_read.shape[0]):\n",
    "    bow_where_text.append(bow_where_read.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_when_read = pd.read_csv('bow_when.csv')\n",
    "bow_when_text = []\n",
    "\n",
    "for i in range(0, bow_when_read.shape[0]):\n",
    "    bow_when_text.append(bow_when_read.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_who_read = pd.read_csv('bow_who.csv')\n",
    "bow_who_text = []\n",
    "\n",
    "for i in range(0, bow_who_read.shape[0]):\n",
    "    bow_who_text.append(bow_who_read.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cari_dokpertama(kueriAsli):\n",
    "    kueriPre=preprocessing(kueriAsli)\n",
    "    kueriPre= \" \".join (kueriPre)\n",
    "    hasilSearch=[]\n",
    "    tfidf_matrix = joblib.load('tfidf_train.pkl')\n",
    "    tfidf_vectorizer = joblib.load('vectorizer.pkl')\n",
    "    query_vec= tfidf_vectorizer.transform([kueriPre])\n",
    "    results=cosine_similarity(tfidf_matrix, query_vec).reshape((-1))\n",
    "    for i in results.argsort()[-5:][::-1]:\n",
    "        hasilSearch.append(df_train.iloc[i,-2])\n",
    "    hasilSearch=\". \".join(hasilSearch)\n",
    "    return hasilSearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Keywords Extraction with YAKE\n",
    "def keyword_yake(hasilSearch):\n",
    "    keywordYake=[]\n",
    "\n",
    "    k_extractor = KeywordExtractor(lan=\"id\", n=1, top=10)\n",
    "    k_extractor2 = KeywordExtractor(lan=\"id\", n=2, top=10)\n",
    "    keywords = k_extractor.extract_keywords(text=hasilSearch)\n",
    "    keywords = k_extractor2.extract_keywords(text=hasilSearch)\n",
    "    keywordYake = [x for x, y in keywords]\n",
    "    #keywordYake.append(keywords)\n",
    "    #print (keywordYake)\n",
    "    return keywordYake\n",
    "#print(\"Keywords of article\\n\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with TFIDF\n",
    "def keyword_tfidf(hasilSearch):\n",
    "\n",
    "    keywordtfidf=[]\n",
    "    keywordtfidf2=[]\n",
    "\n",
    "    #doc = 'بَاب فرض الْوضُوء وسننه وهيآته وَفرض الْوضُوء سِتّ خِصَال النِّيَّة عمند غسل الْوَجْه وَغسل الْوَجْه وَغسل الذراعين مَعَ الْمرْفقين وَمسح مَا قل من الرَّأْس وَغسل الرجلَيْن مَعَ الْكَعْبَيْنِ وَالتَّرْتِيب وعَلى قَول الْوَلَاء وسننه عشر خِصَال خمس مِنْهَا قبل غسل الْوَجْه وَهِي التَّسْمِيَة وَغسل الْكَفَّيْنِ والمضمضة وَالِاسْتِنْشَاق وَالْمُبَالغَة فيههما إِلَّا للصَّائِم وَخمْس بعد غسل الْوَجْه وَهِي تَقْدِيم الْيُمْنَى على ليسرى وَمسح جَمِيع الرَّأْس وَمسح الْأُذُنَيْنِ ظاهرهما وباطنهما وَإِدْخَال الأصبعين فيهمَا وتخليل أَصَابِع الرجلَيْن . وَغسل دَاخل الْكَعْبَيْنِ وَلَيْسَ مسح لعنق من سنَنه وفضيلته تكراره ثَلَاثًا وزالواجب فِيهِ مرّة والمرتان أفضل وَالثَّلَاث أكمل وهيآته أَن يبْدَأ فِي تَطْهِير الْأَعْضَاء بمواضع الِابْتِدَاء . فَإِن اقْتصر على فروضه استة أَجزَأَهُ وَإِن ضيع حَظّ نَفسه فِيمَا ترك'\n",
    "    total_words = re.sub(r'[^\\w]', ' ', hasilSearch)\n",
    "    total_words = total_words.lower().split()\n",
    "    #print (total_words)\n",
    "    total_word_length = len(total_words)\n",
    "    total_sentences = tokenize.sent_tokenize(hasilSearch)\n",
    "    total_sent_len = len(total_sentences)\n",
    "\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in NLTK_StopWords:\n",
    "            if each_word in tf_score:\n",
    "                tf_score[each_word] += 1\n",
    "            else:\n",
    "                tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    def check_sent(word, sentences): \n",
    "        final = [all([w in x for w in word]) for x in sentences] \n",
    "        sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "        return int(len(sent_len))\n",
    "\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #print (each_word)\n",
    "        each_word = each_word.replace('.','')\n",
    "        if each_word not in NLTK_StopWords:\n",
    "            if each_word in idf_score:\n",
    "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "            else:\n",
    "                idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "    #print(idf_score)\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    #print(tf_idf_score)\n",
    "    def get_top_n(dict_elem, n):\n",
    "        result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "        hasil =list(result.keys())\n",
    "        #print(list(result.keys()))        \n",
    "        return hasil\n",
    "    #print(get_top_n(tf_idf_score, 25))\n",
    "    #print(len(get_top_n(tf_idf_score, 1)))\n",
    "    keywordtfidf.append(get_top_n(tf_idf_score, 25))\n",
    "    for i in range(len(keywordtfidf)):\n",
    "        #print (i)\n",
    "        totalKw=0\n",
    "        totalKw=len(keywordtfidf[i])\n",
    "        for j in range(totalKw):\n",
    "            #print (j)\n",
    "            keywordtfidf2.append(keywordtfidf[i][j])\n",
    "    #print (keywordtfidf2)\n",
    "    return keywordtfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords Extraction with BERT\n",
    "def keyword_bert(hasilSearch):\n",
    "\n",
    "    keywordbert=[]\n",
    "\n",
    "    #for j in range(len(array_text)):\n",
    "    keyword1 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 1))\n",
    "    keyword2 = kw_extractor.extract_keywords(hasilSearch, top_n=10, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "    #print(\"Keywords of article\\n\", keywords)\n",
    "    for i in range (0,len (keyword1)):\n",
    "        keywordbert.append(keyword1[i][0])\n",
    "        keywordbert.append(keyword2[i][0])\n",
    "    #print (keywordbert)\n",
    "    return keywordbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rangking (keywordGabung,kueriAsli):\n",
    "    kandidatFinalCek=[]\n",
    "    kandidatFinalFix=[]\n",
    "    for i in keywordGabung:\n",
    "        if (i not in kandidatFinalCek and i!=0):\n",
    "            kandidatFinalCek.append(i)\n",
    "    queries=[kueriAsli]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(kandidatFinalCek)\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 50\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFinalFix.append(kandidatFinalCek[idx])\n",
    "    print ('kandidatFinalFix: ', kandidatFinalFix)\n",
    "    return kandidatFinalFix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_BOW(keywordBOW, kueriAsli):\n",
    "    cekDuplicate = []\n",
    "    kandidatFix = []\n",
    "\n",
    "    for i in keywordBOW:\n",
    "        if(i not in cekDuplicate and i!=0):\n",
    "            cekDuplicate.append(i)\n",
    "\n",
    "    queries=[kueriAsli]\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    corpus_embeddings4 = embedder.encode(cekDuplicate)\n",
    "    \n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    closest_n = 10\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings4, 'cosine')[0]\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        for idx, distance in results[0:closest_n]:\n",
    "            kandidatFix.append(cekDuplicate[idx])\n",
    "    return kandidatFix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kandidatFix(kueriAsli,bow):\n",
    "    \n",
    "    kueri=preprocessing(kueriAsli)\n",
    "    kueri= [\" \".join (kueri)]\n",
    "    print (kueri)\n",
    "    hasilkandidat=[]\n",
    "    keywordGabung=[]\n",
    "    kandidatFix=[]\n",
    "    kueriFix=[]\n",
    "\n",
    "    hasilSearch=cari_dokpertama(kueriAsli)\n",
    "    keywordYake=keyword_yake(hasilSearch)\n",
    "    keywordtfidf2=keyword_tfidf(hasilSearch)\n",
    "    keywordbert=keyword_bert (hasilSearch)\n",
    "    keywordBOW=keyword_BOW(bow, kueriAsli)\n",
    "\n",
    "    for i in keywordYake:\n",
    "        keywordGabung.append(i)\n",
    "    for i in keywordtfidf2:\n",
    "        keywordGabung.append(i)\n",
    "    for i in keywordbert:\n",
    "        keywordGabung.append(i)\n",
    "    for i in keywordBOW:\n",
    "        keywordGabung.append(i)\n",
    "    hasilrank=rangking(keywordGabung,kueriAsli)\n",
    "    # print(hasilrank)\n",
    "    for i in hasilrank:\n",
    "        kueriFix.append(i)\n",
    "    for j in kueriFix:\n",
    "        hasilkandidat.append(j)\n",
    "    kueriFix=[preprocessing(i) for i in kueriFix]\n",
    "    for i in kueriFix:\n",
    "        for j in i:\n",
    "            kandidatFix.append(j)\n",
    "    kandidatFix= [\" \".join (kandidatFix)]\n",
    "    print ('*'*120)\n",
    "\n",
    "    return(kandidatFix,keywordGabung,keywordBOW,hasilrank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diberita']\n",
      "kandidatFinalFix:  ['terwujudnya', 'terselenggaranya', 'masyarakatnya', 'konsumennya', 'nasabahnya', 'tercapainya', 'timbulnya', 'aksinya', 'keputusannya', 'perkembangannya', 'munculnya', 'menganaktirikan', 'ditetapkan menganaktirikan', 'diiringi', 'berhasil', 'sayangnya', 'terbukti', 'pemberlakuan', 'bikin', 'diprediksi', 'penggerak', 'ketentuan', 'dibuktikan', 'ancaman', 'juta orang', 'alami', 'of', 'resesi', 'mengoptimalkan', 'ketergantungan', 'pembatasan', 'kegiatan', 'perekonomian', '07', 'melambat', 'juta', 'lapangan pekerjaan', 'abdullah', 'angka kemiskinan', '7', 'indonesia berhasil', '8', 'perusahaan indonesia', 'reform', '4', 'memulihkan indonesia', 'kemiskinan maret', 'core', 'center', '31']\n",
      "************************************************************************************************************************\n",
      "['daerah kejadian']\n",
      "kandidatFinalFix:  ['konsumsi masyarakat', 'masyarakat', 'maksud', 'temanggung', 'kuin', 'masyarakat tujuan', 'srondol', 'persebaya', 'sumbersuko', 'marabahan', 'keterangannya', 'sanana', 'karangtengah', 'berbasis', 'mataram', 'sangatta', 'lancar', 'dilaksanakan', 'sungguh', 'pelaksanaan', 'mengoptimalkan', 'kabupaten', 'penganekaragaman', 'tindak', 'eksekusi', 'masyarakat firli', 'menyoroti', 'perizinan', 'risiko', 'menimbulkan ketidakpastian', 'pembangunan daerah', 'anggarannya', 'kepala daerah', 'pembangunan', 'pidana', 'komunikasi', 'lihat', 'menggerakkan', 'nya', 'dikembangkan', 'tetaplah', 'keterbukaan', 'ketua', 'pembangunan pertumbuhan', 'kesejahteraan tercapai', 'pendapatan masyarakat', 'rekan rekan', 'rabu', 'kesejahteraan', 'disarankan']\n",
      "************************************************************************************************************************\n",
      "['pelaku kejadian']\n",
      "kandidatFinalFix:  ['pelakunya', 'dipersalahkan', 'kesalahannya', 'konspirator', 'pelaku', 'kemalangan', 'partisipannya', 'korbannya', 'ditimpakan', 'tersangka', 'halnya', 'wakil', 'pelaku usaha', 'mengatasi', 'penggerak', 'diwaspadai', 'dampak', 'disahkan', 'publik', 'pelaku umkm', 'perppu nomor', 'suahasil pandemi', 'lewati', 'merespon', 'menteri', 'zakat', 'melejit', 'kabupaten', 'penanganan pandemi', 'bangkit', 'sistem', 'menengah', 'upaya', 'usaha', 'juli', 'meningkatkan', 'negeri', 'menerbitkan', '19', 'jakarta', 'stabilitas', 'webinar', 'keterbukaan', 'umkm', 'agustus', '8', 'pandemi covid', '2022 kesempatan', 'nomor 2020', 'pandemi corona']\n",
      "************************************************************************************************************************\n",
      "['waktu kejadian']\n",
      "kandidatFinalFix:  ['kapankah', 'apakah', 'kenapa', 'ketika', 'dizaman', 'apabila', 'sebabnya', 'masanya', 'saatnya', 'diwaktu', 'jangka waktu', 'halnya', 'menimbulkan', 'salah', 'suahasil penanganan', 'disahkan', 'wakil', 'publik', 'acara', 'lewati', 'kegentingan antisipasi', 'kabupaten', 'diwaspadai', 'cepat', 'merespon', 'menginstruksikan', 'melejit', 'pembangunan', 'negeri', 'menerbitkan', 'jakarta', 'juli', 'keuangan', 'upaya', 'keterbukaan', 'agustus', '19', '97', 'webinar', '8', 'corona suahasil', 'stabilitas', '1997', '6', '2022', 'informasi publik', '2021 mencapai', 'penanganan corona', 'krisis keuangan', '2021']\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "hasilDokumenWhat=[]\n",
    "hasilDokumenWho=[]\n",
    "hasilDokumenWhen=[]\n",
    "hasilDokumenWhere=[]\n",
    "\n",
    "kueriAsliWhat='apa yang terjadi diberita tersebut'\n",
    "kueriAsliWhere='di daerah mana kejadian itu terjadi'\n",
    "kueriAsliWho='siapa pelaku kejadian ini'\n",
    "kueriAsliWhen='Kapan waktu kejadian tersebut'\n",
    "\n",
    "kandidatFixWhat,keywordGabungWhat,keywordBOW_What, hasilrankWhat = kandidatFix(kueriAsliWhat, bow_ekonomi_text)\n",
    "kandidatFixWhere,keywordGabungWhere,keywordBOW_Where, hasilrankWhere = kandidatFix(kueriAsliWhere, bow_where_text)\n",
    "kandidatFixWho,keywordGabungWho,keywordBOW_Who, hasilrankWho = kandidatFix(kueriAsliWho, bow_who_text)\n",
    "kandidatFixWhen,keywordGabungWhen,keywordBOW_When, hasilrankWhen = kandidatFix(kueriAsliWhen, bow_when_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #kueri what\n",
    "# kueriAsliWhat='apa sebenarnya kejadian yang terjadi diberita tersebut'\n",
    "# kueriWhat=preprocessing(kueriAsliWhat)\n",
    "# kueriWhat= [\" \".join (kueriWhat)]\n",
    "# print (kueriWhat)\n",
    "# hasilkandidatWhat=[]\n",
    "# keywordGabungWhat=[]\n",
    "# kandidatFixWhat=[]\n",
    "# kueriFixWhat=[]\n",
    "# hasilDokumenWhat=[]\n",
    "\n",
    "# hasilSearchWhat=cari_dokpertama(kueriAsliWhat)\n",
    "# keywordYakeWhat=keyword_yake(hasilSearchWhat)\n",
    "# keywordtfidf2What=keyword_tfidf(hasilSearchWhat)\n",
    "# keywordbertWhat=keyword_bert (hasilSearchWhat)\n",
    "# keywordBOW_What=keyword_BOW(bow_kecelakaan_text, kueriAsliWhat)\n",
    "\n",
    "# for i in keywordYakeWhat:\n",
    "#     keywordGabungWhat.append(i)\n",
    "# for i in keywordtfidf2What:\n",
    "#     keywordGabungWhat.append(i)\n",
    "# for i in keywordbertWhat:\n",
    "#     keywordGabungWhat.append(i)\n",
    "# for i in keywordBOW_What:\n",
    "#     keywordGabungWhat.append(i)\n",
    "# hasilrankWhat=rangking(keywordGabungWhat,kueriAsliWhat)\n",
    "# # print(hasilrank)\n",
    "# for i in hasilrankWhat:\n",
    "#     kueriFixWhat.append(i)\n",
    "# for j in kueriFixWhat:\n",
    "#     hasilkandidatWhat.append(j)\n",
    "# kueriFixWhat=[preprocessing(i) for i in kueriFixWhat]\n",
    "# for i in kueriFixWhat:\n",
    "#     for j in i:\n",
    "#         kandidatFixWhat.append(j)\n",
    "# kandidatFixWhat= [\" \".join (kandidatFixWhat)]\n",
    "# print ('*'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #kueri where\n",
    "# kueriAsliWhere='di daerah mana kejadian itu terjadi'\n",
    "# kueriWhere=preprocessing(kueriAsliWhere)\n",
    "# kueriWhere= [\" \".join (kueriWhere)]\n",
    "# print (kueriWhere)\n",
    "# hasilkandidatWhere=[]\n",
    "# keywordGabungWhere=[]\n",
    "# kandidatFixWhere=[]\n",
    "# kueriFixWhere=[]\n",
    "# hasilDokumenWhere=[]\n",
    "\n",
    "# hasilSearchWhere=cari_dokpertama(kueriAsliWhere)\n",
    "# keywordYakeWhere=keyword_yake(hasilSearchWhere)\n",
    "# keywordtfidf2Where=keyword_tfidf(hasilSearchWhere)\n",
    "# keywordbertWhere=keyword_bert (hasilSearchWhere)\n",
    "# keywordBOW_Where=keyword_BOW(bow_where_text, kueriAsliWhere)\n",
    "\n",
    "# for i in keywordYakeWhere:\n",
    "#     keywordGabungWhere.append(i)\n",
    "# for i in keywordtfidf2Where:\n",
    "#     keywordGabungWhere.append(i)\n",
    "# for i in keywordbertWhere:\n",
    "#     keywordGabungWhere.append(i)\n",
    "# for i in keywordBOW_Where:\n",
    "#     keywordGabungWhere.append(i)\n",
    "# hasilrankWhere=rangking(keywordGabungWhere,kueriAsliWhere)\n",
    "# # print(hasilrank)\n",
    "# for i in hasilrankWhere:\n",
    "#     kueriFixWhere.append(i)\n",
    "# for j in kueriFixWhere:\n",
    "#     hasilkandidatWhere.append(j)\n",
    "# kueriFixWhere=[preprocessing(i) for i in kueriFixWhere]\n",
    "# for i in kueriFixWhere:\n",
    "#     for j in i:\n",
    "#         kandidatFixWhere.append(j)\n",
    "# kandidatFixWhere= [\" \".join (kandidatFixWhere)]\n",
    "# print ('*'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #kueri who\n",
    "# kueriAsliWho='siapa pelaku kejadian ini'\n",
    "# kueriWho=preprocessing(kueriAsliWho)\n",
    "# kueriWho= [\" \".join (kueriWho)]\n",
    "# print (kueriWho)\n",
    "# hasilkandidatWho=[]\n",
    "# keywordGabungWho=[]\n",
    "# kandidatFixWho=[]\n",
    "# kueriFixWho=[]\n",
    "# hasilDokumenWho=[]\n",
    "\n",
    "# hasilSearchWho=cari_dokpertama(kueriAsliWho)\n",
    "# keywordYakeWho=keyword_yake(hasilSearchWho)\n",
    "# keywordtfidf2Who=keyword_tfidf(hasilSearchWho)\n",
    "# keywordbertWho=keyword_bert (hasilSearchWho)\n",
    "# keywordBOW_Who=keyword_BOW(bow_who_text, kueriAsliWho)\n",
    "\n",
    "# for i in keywordYakeWho:\n",
    "#     keywordGabungWho.append(i)\n",
    "# for i in keywordtfidf2Who:\n",
    "#     keywordGabungWho.append(i)\n",
    "# for i in keywordbertWho:\n",
    "#     keywordGabungWho.append(i)\n",
    "# for i in keywordBOW_Who:\n",
    "#     keywordGabungWho.append(i)\n",
    "# hasilrankWho=rangking(keywordGabungWho,kueriAsliWho)\n",
    "# # print(hasilrank)\n",
    "# for i in hasilrankWho:\n",
    "#     kueriFixWho.append(i)\n",
    "# for j in kueriFixWho:\n",
    "#     hasilkandidatWho.append(j)\n",
    "# kueriFixWho=[preprocessing(i) for i in kueriFixWho]\n",
    "# for i in kueriFixWho:\n",
    "#     for j in i:\n",
    "#         kandidatFixWho.append(j)\n",
    "# kandidatFixWho= [\" \".join (kandidatFixWho)]\n",
    "# print ('*'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #kueri when\n",
    "# kueriAsliWhen='Kapan waktu kejadian tersebut'\n",
    "# kueriWhen=preprocessing(kueriAsliWhen)\n",
    "# kueriWhen= [\" \".join (kueriWhen)]\n",
    "# print (kueriWhen)\n",
    "# hasilkandidatWhen=[]\n",
    "# keywordGabungWhen=[]\n",
    "# kandidatFixWhen=[]\n",
    "# kueriFixWhen=[]\n",
    "# hasilDokumenWhen=[]\n",
    "\n",
    "# hasilSearchWhen=cari_dokpertama(kueriAsliWhen)\n",
    "# keywordYakeWhen=keyword_yake(hasilSearchWhen)\n",
    "# keywordtfidf2When=keyword_tfidf(hasilSearchWhen)\n",
    "# keywordbertWhen=keyword_bert (hasilSearchWhen)\n",
    "# keywordBOW_When=keyword_BOW(bow_when_text, kueriAsliWhen)\n",
    "\n",
    "# for i in keywordYakeWhen:\n",
    "#     keywordGabungWhen.append(i)\n",
    "# for i in keywordtfidf2When:\n",
    "#     keywordGabungWhen.append(i)\n",
    "# for i in keywordbertWhen:\n",
    "#     keywordGabungWhen.append(i)\n",
    "# for i in keywordBOW_When:\n",
    "#     keywordGabungWhen.append(i)\n",
    "# hasilrankWhen=rangking(keywordGabungWhen,kueriAsliWhen)\n",
    "# # print(hasilrank)\n",
    "# for i in hasilrankWhen:\n",
    "#     kueriFixWhen.append(i)\n",
    "# for j in kueriFixWhen:\n",
    "#     hasilkandidatWhen.append(j)\n",
    "# kueriFixWhen=[preprocessing(i) for i in kueriFixWhen]\n",
    "# for i in kueriFixWhen:\n",
    "#     for j in i:\n",
    "#         kandidatFixWhen.append(j)\n",
    "# kandidatFixWhen= [\" \".join (kandidatFixWhen)]\n",
    "# print ('*'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "\n",
    "testing_data = []\n",
    "for i in range(0, len(document_text_test)-1):\n",
    "    \n",
    "    hasilWhat=[]\n",
    "\n",
    "    teks=df_test.iloc[i,-2]\n",
    "    tfidf_vectorizer = joblib.load('vectorizer.pkl')\n",
    "    # tfidf_matrix = joblib.load('tfidf_test.pkl')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([teks])\n",
    "\n",
    "    query_vec_What= tfidf_vectorizer.transform(kandidatFixWhat)\n",
    "    results_what=cosine_similarity(tfidf_matrix, query_vec_What).reshape((-1))\n",
    "    hasilDokumenWhat.append(df_test.iloc[i,2])\n",
    "    for a in hasilrankWhat:\n",
    "        cariW = re.findall(a,hasilDokumenWhat[i])\n",
    "        #print(cariW)\n",
    "        if cariW:\n",
    "            hasilWhat.append(a)\n",
    "    \n",
    "    data = [i,df_test.iloc[i,2],'what',kueriAsliWhat,keywordBOW_What , keywordGabungWhat, hasilWhat, results_what,' ',' ']\n",
    "\n",
    "    testing_data.append(data)\n",
    "\n",
    "\n",
    "for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "    hasilWhere=[]\n",
    "\n",
    "    query_vec_Where= tfidf_vectorizer.transform(kandidatFixWhere)\n",
    "    results_where=cosine_similarity(tfidf_matrix, query_vec_Where).reshape((-1))\n",
    "    hasilDokumenWhere.append(df_test.iloc[i,2])\n",
    "    for a in hasilrankWhere:\n",
    "        cariW = re.findall(a,hasilDokumenWhere[i])\n",
    "        #print(cariW)\n",
    "        if cariW:\n",
    "            hasilWhere.append(a)\n",
    "    \n",
    "    data = [i,df_test.iloc[i,2],'where',kueriAsliWhere,keywordBOW_Where , keywordGabungWhere, hasilWhere, results_where,' ',' ']\n",
    "\n",
    "    testing_data.append(data)\n",
    "\n",
    "\n",
    "for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "    hasilWho=[]\n",
    "\n",
    "    query_vec_Who= tfidf_vectorizer.transform(kandidatFixWho)\n",
    "    results_who=cosine_similarity(tfidf_matrix, query_vec_Who).reshape((-1))\n",
    "    hasilDokumenWho.append(df_test.iloc[i,2])\n",
    "    for a in hasilrankWho:\n",
    "        cariW = re.findall(a,hasilDokumenWho[i])\n",
    "        #print(cariW)\n",
    "        if cariW:\n",
    "            hasilWho.append(a)\n",
    "    \n",
    "    data = [i,df_test.iloc[i,2],'who',kueriAsliWho,keywordBOW_Who , keywordGabungWho, hasilWho, results_who,' ',' ']\n",
    "\n",
    "    testing_data.append(data)\n",
    "\n",
    "for i in range(0, len(document_text_test)-1):\n",
    "\n",
    "    hasilWhen=[]\n",
    "\n",
    "    query_vec_When= tfidf_vectorizer.transform(kandidatFixWhen)\n",
    "    results_when=cosine_similarity(tfidf_matrix, query_vec_When).reshape((-1))\n",
    "    hasilDokumenWhen.append(df_test.iloc[i,2])\n",
    "    for a in hasilrankWhen:\n",
    "        cariW = re.findall(a,hasilDokumenWhen[i])\n",
    "        #print(cariW)\n",
    "        if cariW:\n",
    "            hasilWhen.append(a)\n",
    "    \n",
    "    data = [i,df_test.iloc[i,2],'when',kueriAsliWhen,keywordBOW_When , keywordGabungWhen, hasilWhen, results_when,' ',' ']\n",
    "\n",
    "    testing_data.append(data)\n",
    "    \n",
    "    # print (j)\n",
    "    # print(\"No ID Dokumen  : \", i)\n",
    "    # print(\"Tanggal        : \", df_test.iloc[i,1])\n",
    "    # print(\"Isi berita     : \", df_test.iloc[i,2])\n",
    "    # print (\"Hasil W       : \",hasilW)\n",
    "    # print(\"(Kemiripan: %.4f) \" % results)\n",
    "    # print ('*'*120)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv\n",
    "writer = pd.DataFrame(testing_data, columns=['No Document','Description', 'W','Keyword W', 'Keyword BOW', 'Keyword Gabung','hasilW', 'Kemiripan', 'True Positif', 'True Negative'])\n",
    "writer.to_csv('QE_Stat_V2_testing_result.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kueriAsli='kecelakaan'\n",
    "# kueri=preprocessing(kueriAsli)\n",
    "# kueri= [\" \".join (kueri)]\n",
    "# print (kueri)\n",
    "# hasilkandidat=[]\n",
    "# keywordGabung=[]\n",
    "# kandidatFix=[]\n",
    "# kueriFix=[]\n",
    "# hasilDokumen=[]\n",
    "# hasilSearch=cari_dokpertama(kueriAsli)\n",
    "# keywordYake=keyword_yake(hasilSearch)\n",
    "# keywordtfidf2=keyword_tfidf(hasilSearch)\n",
    "# keywordbert=keyword_bert (hasilSearch)\n",
    "# keywordBOW=keyword_BOW(bow_text, kueriAsli)\n",
    "# for i in keywordYake:\n",
    "#     keywordGabung.append(i)\n",
    "# for i in keywordtfidf2:\n",
    "#     keywordGabung.append(i)\n",
    "# for i in keywordbert:\n",
    "#     keywordGabung.append(i)\n",
    "# for i in keywordBOW:\n",
    "#     keywordGabung.append(i)\n",
    "# hasilrank=rangking(keywordGabung,kueriAsli)\n",
    "# for i in hasilrank:\n",
    "#     kueriFix.append(i)\n",
    "# for j in kueriFix:\n",
    "#     hasilkandidat.append(j)\n",
    "# kueriFix=[preprocessing(i) for i in kueriFix]\n",
    "# for i in kueriFix:\n",
    "#     for j in i:\n",
    "#         kandidatFix.append(j)\n",
    "# kandidatFix= [\" \".join (kandidatFix)]\n",
    "# print ('*'*120)\n",
    "# tfidf_matrix =joblib.load( \"tfidf_test.pkl\" )\n",
    "# tfidf_vectorizer = joblib.load( \"vectorizer.pkl\" ) \n",
    "# query_vec= tfidf_vectorizer.transform(kandidatFix)\n",
    "# results=cosine_similarity(tfidf_matrix, query_vec).reshape((-1))\n",
    "# j=1\n",
    "# for i in results.argsort()[-10:][::-1]:\n",
    "#     print (j)\n",
    "#     print(\"No ID Dokumen  : \", i)\n",
    "#     print(\"Tanggal        : \", df_test.iloc[i,1])\n",
    "#     print(\"Isi berita     : \", df_test.iloc[i,2])\n",
    "#     print(\"(Kemiripan: %.4f) \" % results[i])\n",
    "#     hasilDokumen.append(df_test.iloc[i,2])\n",
    "#     print ('*'*120)\n",
    "#     j+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
